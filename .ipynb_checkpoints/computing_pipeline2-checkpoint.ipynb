{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "\n",
    "# %autoreload 4\n",
    "from sqlalchemy import Table, Column, String, Integer, Float, Boolean, MetaData, insert, select, BIGINT, Date, DateTime, VARCHAR\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "def get_filtered_fg_df(feature_engineered_df):\n",
    "    item_ids_static = feature_engineered_df.item_id[(feature_engineered_df.std_in_cluster == 0.0)].values\n",
    "    data_df_cleaned = feature_engineered_df[feature_engineered_df.mean_in_cluster.notnull()]\n",
    "    purified_df = data_df_cleaned[(data_df_cleaned.ratio_drop < 0.2)\n",
    "                          & (data_df_cleaned.ratio_same_value < 0.3)\n",
    "                          & (data_df_cleaned.n_jumps < 2)\n",
    "                          & (data_df_cleaned.n_days > 5)\n",
    "                          & (data_df_cleaned.std_in_cluster > 0.2)\n",
    "                          & (data_df_cleaned.std_in_cluster < 4)\n",
    "                          & (data_df_cleaned.ratio_of_na < 0.5)\n",
    "                          & (data_df_cleaned.n_unique_stock_id < 30)]\n",
    "    return purified_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "epopcon_db = Epopcon_db()\n",
    "\n",
    "wspider_eng = epopcon_db.get_engine(production=True)\n",
    "wspider_temp_eng = epopcon_db.get_engine(production=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batches, save_db=True):\n",
    "    \n",
    "    idx, query = batches\n",
    "    \n",
    "    logging.warning('wokring on %s' % str(idx))\n",
    "    batch = pd.read_sql_query(\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE ITEM_ID in %s\" % query, wspider_eng)\n",
    "    \n",
    "    \n",
    "    result_lst = []\n",
    "    for idx, group_by_item_id in batch.groupby('ITEM_ID'):\n",
    "        tmp = list(group_by_item_id.groupby('STOCK_ID'))[0][1]    \n",
    "        result_lst.append(get_feature_engineered_bundle(tmp))\n",
    "\n",
    "\n",
    "    results = [result for result in result_lst if result != None]\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # save feature engineered df\n",
    "#     result_df.to_pickle('data/pickle/ivt_item_feature_engineered/%s' % str(file.split('/')[-1]))\n",
    "    \n",
    "    # filter dataframe\n",
    "    try:\n",
    "        filtered_df = get_filtered_fg_df(result_df)\n",
    "\n",
    "    \n",
    "        cleaned_item_ids = filtered_df.item_id.values\n",
    "        cleaned_df = batch[batch['ITEM_ID'].isin(cleaned_item_ids)]\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_lst =[]\n",
    "    \n",
    "#     save images\n",
    "#     save_img(cleaned_df)\n",
    "\n",
    "    if save_db:\n",
    "\n",
    "        for idx, group in cleaned_df.groupby('ITEM_ID'):\n",
    "            try:\n",
    "                df_lst.append(get_sell_amount_by_item_id(group))\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "        if len(df_lst) > 0:\n",
    "\n",
    "            result = pd.concat(df_lst)\n",
    "            result[['COLLECT_DAY']] = result.index\n",
    "\n",
    "#             del result['STOCK_AMOUNT_imputed']\n",
    "#             del result['STOCK_AMOUNT']\n",
    "\n",
    "            result.to_sql(con=wspider_temp_eng, name='MWS_COLT_ITEM_SELL_AMT_DEV', if_exists='append')\n",
    "\n",
    "            logging.warning('done with %s' % str(file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sell_amount_by_item_id(df, add_sell_amount=False):\n",
    "#     print('hierer')\n",
    "    collect_day = df.COLLECT_DAY.values[0]\n",
    "    reg_id = df.REG_ID.values[0]\n",
    "    \n",
    "    tmp_lst = []\n",
    "    for stock_id, group_df in list(df.groupby('STOCK_ID')):\n",
    "        tmp_lst.append(map_clean_up_target_df(stock_id, group_df))    \n",
    "    result = pd.concat(tmp_lst)\n",
    "    \n",
    "    \n",
    "#     df_pivot = df.pivot_table(index='REG_DT', columns='STOCK_ID', values='STOCK_AMOUNT')\n",
    "#     sell_amount_by_stock = df_pivot.apply(map_clean_up_target_df)\n",
    "\n",
    "#     if add_sell_amount:\n",
    "#         sell_amount_total = sell_amount_by_stock.sum(axis=1)\n",
    "#         result = pd.DataFrame(sell_amount_total)\n",
    "#         result.columns = ['SELL_AMOUNT']\n",
    "#         result['REG_ID'] = reg_id\n",
    "#     else:\n",
    "#         sell_amount_by_stock['REG_DT'] = sell_amount_by_stock.index\n",
    "#         result = pd.melt(sell_amount_by_stock, id_vars=[\"REG_DT\"], var_name=\"STOCK_ID\", value_name=\"SELL_AMOUNT\")\n",
    "\n",
    "    item_id = df.ITEM_ID.values[0]\n",
    "    result['ITEM_ID'] = item_id\n",
    "    result['REG_ID'] = reg_id\n",
    "    result['UPT_DT'] = pd.to_datetime('now')\n",
    "    result['COLLECT_DAY'] = collect_day\n",
    "    result['UPT_ID'] = 'FILTER ALGO'\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clean_up_target_df(stock_id, group_df):\n",
    "\n",
    "    tmp_df = clean_up_target_df(group_df)[['sell_impute', 'STOCK_AMOUNT', 'STOCK_AMOUNT_imputed']]\n",
    "#     tmp_df = clean_up_target_df(group_df)[['SELL_AMOUNT']]\n",
    "    tmp_df['STOCK_ID'] = stock_id\n",
    "#     tmp_df.columns = ['SELL_AMOUNT', 'STOCK_ID']\n",
    "    tmp_df.columns = ['SELL_AMOUNT', 'STOCK_AMOUNT', 'REVISE_STOCK_AMOUNT', 'STOCK_ID']\n",
    "\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_df = pd.read_sql_query(\"SELECT ID FROM MWS_COLT_ITEM WHERE RELEASE_DT > '2018-01-01'\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENOM = 100\n",
    "item_ids = ids_df.ID.values[:1000]\n",
    "n_batches = math.ceil( len(item_ids) / float(DENOM))\n",
    "batch_ls = [str(tuple(batch)) for batch in np.array_split(item_ids, n_batches)]\n",
    "batch_lst = [(idx, row) for idx, row in enumerate(batch_ls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_batch(batch_lst[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def save_ivt_to_pickle(batch_lst, by=1000):\n",
    "Parallel(n_jobs=-1)(map(delayed(process_batch), batch_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
