{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from time import gmtime, strftime\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "from sqlalchemy import ForeignKey, Table, Column, String, Integer, Float, Boolean, MetaData, select\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Epopcon_db:\n",
    "    def __init__(self, local_access=True):\n",
    "        \n",
    "        if local_access:\n",
    "        \n",
    "            self.wspider_temp = create_engine(\"mysql://eums:eums00!q@115.90.182.250:11000/wspider_temp\", pool_size=20, pool_recycle=3600,\n",
    "                           connect_args={'connect_timeout': 1000000})\n",
    "            self.wspider = create_engine(\"mysql://wspider:wspider00!q@192.168.0.36:3306/wspider\", pool_size=20, pool_recycle=3600,\n",
    "                       connect_args={'connect_timeout': 1000000})\n",
    "        else:\n",
    "            \n",
    "            self.wspider_temp = create_engine(\"mysql://eums:eums00!q@192.168.0.50:3306/wspider_temp\", pool_size=20, pool_recycle=3600,\n",
    "                           connect_args={'connect_timeout': 1000000})\n",
    "            \n",
    "            self.wspider = create_engine(\"mysql://wspider:wspider00!q@133.186.143.65:3306/wspider\", pool_size=20, pool_recycle=3600,\n",
    "                           connect_args={'connect_timeout': 1000000})\n",
    "                \n",
    "        add_engine_pidguard(self.wspider_temp)\n",
    "        add_engine_pidguard(self.wspider)\n",
    "    def get_engine(self, production=False):\n",
    "        \n",
    "        if production:\n",
    "            return self.wspider\n",
    "        else:\n",
    "            return self.wspider_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epopcon_db = Epopcon_db()\n",
    "\n",
    "wspider_engine = epopcon_db.get_engine(production=True)\n",
    "wspider_temp_engine = epopcon_db.get_engine(production=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(target):\n",
    "    \n",
    "    # cluster inventory data points\n",
    "    n_cluster, label = get_label_from_dbscan(target, eps=0.15, min_samples=3)\n",
    "    target['label'] = label\n",
    "    target = target[['STOCK_AMOUNT', 'label', 'REG_DT']]\n",
    "    labels = target.label.unique()\n",
    "    \n",
    "    # resample to a daily scale\n",
    "    target = target.set_index('REG_DT')\n",
    "    target = target.resample('1D').first()\n",
    "    \n",
    "    # placeholding\n",
    "    target['STOCK_AMOUNT_imputed'] = target['STOCK_AMOUNT']\n",
    "\n",
    "    # interpolate data points based on cluster group\n",
    "    for label in labels:\n",
    "        idx = np.where(target.label.values == label)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        start_v = min(idx)\n",
    "        end_v = max(idx)\n",
    "        target.loc[start_v:end_v+1, 'STOCK_AMOUNT_imputed'] = target['STOCK_AMOUNT'][start_v:end_v+1].interpolate(method='from_derivatives')\n",
    "\n",
    "    # interpolate data points based on global data points\n",
    "    target['STOCK_AMOUNT_imputed'] = target['STOCK_AMOUNT'].interpolate(method='from_derivatives')\n",
    "    \n",
    "    # round STOCK_AMOUNT_imputed to make it cleaner\n",
    "    target['STOCK_AMOUNT_imputed'] = target.STOCK_AMOUNT_imputed.round()\n",
    "\n",
    "    # calculate sell amount \n",
    "    target['sell'] = np.append([0], np.negative(np.diff(target.STOCK_AMOUNT_imputed)))\n",
    "    target.loc[target['sell'].values < 0, 'sell'] = np.nan\n",
    "    target.sell.astype(float)\n",
    "    \n",
    "    # calculate z-score for thresholding\n",
    "    target['zscore'] = np.abs(target.sell - target.sell.mean() / max(0.0001, target.sell.std()))\n",
    "\n",
    "    # get rid of outliers \n",
    "    target.loc[target['zscore'] > 4, 'sell'] = np.nan\n",
    "    \n",
    "    # prepare matrix for data imputation using KNN based on dayofweek\n",
    "    target['weekday_name'] = target.index.dayofweek\n",
    "    X_incomplete = target[['sell', 'weekday_name']].values\n",
    "\n",
    "    # run KNN to calculate sell_impute (imputed version of sell amount)\n",
    "    try:\n",
    "        X_filled_knn = KNN(k=1).complete(X_incomplete)\n",
    "        target['sell_impute'] = X_filled_knn[:,0]\n",
    "    except:\n",
    "        target['sell_impute'] = target['sell']\n",
    "    \n",
    "    # placeholding\n",
    "    target['STOCK_AMOUNT_imputed_trimed'] = target['STOCK_AMOUNT_imputed']\n",
    "    \n",
    "    # get rid of jumpbs\n",
    "    cond = np.append([0], np.negative(np.diff(target.STOCK_AMOUNT_imputed))) < 0\n",
    "    target.loc[cond, 'STOCK_AMOUNT_imputed_trimed'] = np.nan\n",
    "\n",
    "    return target\n",
    "\n",
    "# TODO optimize parameters using ML\n",
    "\n",
    "def get_filtered_fg_df(feature_engineered_df):\n",
    "    static_item_ids = feature_engineered_df.item_id[(feature_engineered_df.std_in_cluster == 0.0)].values\n",
    "    data_df_cleaned = feature_engineered_df[feature_engineered_df.mean_in_cluster.notnull()]\n",
    "    purified_df = data_df_cleaned[(data_df_cleaned.ratio_drop < 0.3)\n",
    "#                           & (data_df_cleaned.ratio_same_value < 0.3)\n",
    "                          & (data_df_cleaned.n_jumps <= 3)\n",
    "                          & (data_df_cleaned.n_days >= 3)\n",
    "#                           & (data_df_cleaned.std_in_cluster > 0.2)\n",
    "                          & (data_df_cleaned.std_in_cluster < 4)\n",
    "                          & (data_df_cleaned.ratio_of_na < 0.5)\n",
    "#                           & (data_df_cleaned.n_unique_stock_id < 50)\n",
    "                                 ]\n",
    "    return purified_df, static_item_ids\n",
    "\n",
    "def get_sell_amount_by_item_id(df, add_sell_amount=False):\n",
    "    \n",
    "    collect_day = df.COLLECT_DAY.values[0]\n",
    "    reg_id = df.REG_ID.values[0]\n",
    "    \n",
    "    imputed_df_lst = []\n",
    "    for stock_id, group_df in list(df.groupby('STOCK_ID')):\n",
    "        \n",
    "        imputed_df = impute_data(group_df)[['sell_impute', 'STOCK_AMOUNT', 'STOCK_AMOUNT_imputed_trimed']]\n",
    "        imputed_df['STOCK_ID'] = stock_id        \n",
    "        imputed_df_lst.append(imputed_df)\n",
    "        \n",
    "    imputed_df = pd.concat(imputed_df_lst)\n",
    "    imputed_df.columns = ['SELL_AMOUNT', 'STOCK_AMOUNT', 'REVISE_STOCK_AMOUNT', 'STOCK_ID']\n",
    "    imputed_df['ITEM_ID'] = df.ITEM_ID.values[0]\n",
    "    imputed_df['REG_ID'] = reg_id\n",
    "    imputed_df['UPT_DT'] = pd.to_datetime(datetime.now(timezone('Asia/Seoul')).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    imputed_df['COLLECT_DAY'] = collect_day\n",
    "    imputed_df['UPT_ID'] = 'FILTER ALGO'\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "def insert_extracted_feature(extracted_feature_df):\n",
    "    extracted_feature_df = extracted_feature_df.where((pd.notnull(extracted_feature_df)), None)\n",
    "    query = \"\"\"REPLACE INTO MWS_COLT_ITEM_EXTRACTED_FEATURE %s VALUES %s \"\"\" % (tuple(extracted_feature_df.columns), tuple(['%s' for _ in range(len(extracted_feature_df.columns))]))\n",
    "    query = query.replace(\"'\", \"\")\n",
    "    wspider_temp_engine.execute(query, [tuple(x) for x in extracted_feature_df.values])\n",
    "\n",
    "def insert_sell_amt(sell_amt_df):\n",
    "    sell_amt_df = sell_amt_df.where((pd.notnull(sell_amt_df)), None)\n",
    "    query = \"\"\"REPLACE INTO MWS_COLT_ITEM_SELL_AMT_DEV %s VALUES %s \"\"\" % (tuple(sell_amt_df.columns), tuple(['%s' for _ in range(len(sell_amt_df.columns))]))\n",
    "    query = query.replace(\"'\", \"\")\n",
    "    wspider_temp_engine.execute(query, [tuple(x) for x in sell_amt_df.values])\n",
    "    \n",
    "    query2 = \"\"\"REPLACE INTO MWS_COLT_ITEM_SELL_AMT %s VALUES %s \"\"\" % (tuple(sell_amt_df.columns), tuple(['%s' for _ in range(len(sell_amt_df.columns))]))\n",
    "    query2 = query2.replace(\"'\", \"\")\n",
    "    wspider_engine.execute(query2, [tuple(x) for x in sell_amt_df.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_full_batch(batches, save_db=True, save_img=False, save_fe=True):\n",
    "    try:\n",
    "        # select multiple items\n",
    "        idx, query = batches\n",
    "        batch = pd.read_sql_query(\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE ITEM_ID in %s\" % query, wspider_engine)\n",
    "\n",
    "        # extract features by stock id\n",
    "        result_lst = []\n",
    "        for idx, group_by_item_id in batch.groupby('ITEM_ID'):\n",
    "            tmp = list(group_by_item_id.groupby('STOCK_ID'))[0][1]    \n",
    "            result_lst.append(get_feature_engineered_bundle(tmp))\n",
    "\n",
    "        # clean up extracted feature df\n",
    "        extracted_feature_df = pd.DataFrame([result for result in result_lst if result != None])\n",
    "\n",
    "\n",
    "        try:\n",
    "            # filter dataframe based on extraction criteria\n",
    "            filtered_df, static_item_ids = get_filtered_fg_df(extracted_feature_df)\n",
    "\n",
    "            # filtered df\n",
    "            cleaned_item_ids = filtered_df.item_id.values\n",
    "            cleaned_df = batch[batch['ITEM_ID'].isin(cleaned_item_ids)]\n",
    "\n",
    "            # label extracted feature df\n",
    "            extracted_feature_df['condition_clean'] = 0\n",
    "            extracted_feature_df.loc[extracted_feature_df.item_id.isin(cleaned_item_ids), 'condition_clean'] = 1\n",
    "            extracted_feature_df.loc[extracted_feature_df.item_id.isin(static_item_ids), 'condition_clean'] = 2\n",
    "\n",
    "\n",
    "        except:\n",
    "            return\n",
    "\n",
    "        # save images\n",
    "        if save_img:\n",
    "            save_img(cleaned_df)\n",
    "\n",
    "        # save extracted features to db\n",
    "        if save_fe:\n",
    "\n",
    "            insert_extracted_feature(extracted_feature_df)\n",
    "\n",
    "\n",
    "        if save_db:\n",
    "\n",
    "            df_lst =[]\n",
    "\n",
    "            for idx, group in cleaned_df.groupby('ITEM_ID'):\n",
    "                try:\n",
    "                    df_lst.append(get_sell_amount_by_item_id(group))\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "\n",
    "            if len(df_lst) > 0:\n",
    "\n",
    "                result = pd.concat(df_lst)\n",
    "                result[['COLLECT_DAY']] = result.index\n",
    "                insert_sell_amt(result)\n",
    "    #             result.to_sql(con=wspider_temp_engine, name='MWS_COLT_ITEM_SELL_AMT_DEV', if_exists='append')\n",
    "    #             logging.warning('done with %s' % str(file))\n",
    "\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_df = pd.read_sql_query(\"SELECT ID FROM MWS_COLT_ITEM WHERE RELEASE_DT > '2018-01-01'\", wspider_engine)\n",
    "\n",
    "DENOM = 500\n",
    "item_ids = ids_df.ID.values\n",
    "n_batches = math.ceil( len(item_ids) / float(DENOM))\n",
    "batch_ls = [str(tuple(batch)) for batch in np.array_split(item_ids, n_batches)]\n",
    "batch_lst = [(idx, row) for idx, row in enumerate(batch_ls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query = \"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN %s\" % batch_lst[6][1]\n",
    "\n",
    "# cursor.execute(query)\n",
    "\n",
    "# tmppp = as_pandas(cursor)\n",
    "# elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "query = batch_lst[6][1]\n",
    "batch = pd.read_sql_query(\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE ITEM_ID in %s\" % query, wspider_engine)\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.911540031433105"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=-1)(map(delayed(process_full_batch), batch_lst[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=-1)(map(delayed(process_full_batch), batch_lst[1000:2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=-1)(map(delayed(process_full_batch), batch_lst[2000:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=-1)(map(delayed(process_full_batch), batch_lst[3000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fe5730ab110>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"\"\"REPLACE INTO ADDRESS (USER_ID, EMAIL_ADDRESS) VALUES (3, 'Akaj119@naver.com')\"\"\"\n",
    "# # query = query.replace(\"'\", \"\")\n",
    "\n",
    "# wspider_temp_engine.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"REPLACE INTO MWS_COLT_ITEM_SELL_AMT_DEV %s VALUES %s \"\"\" % (tuple(sell_amt_df.columns), tuple(['%s' for _ in range(len(sell_amt_df.columns))]))\n",
    "#     query = query.replace(\"'\", \"\")\n",
    "#     wspider_temp_engine.execute(query, [tuple(x) for x in sell_amt_df.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENOM = 50\n",
    "# item_ids = ids_df.ID.values[:1000]\n",
    "# n_batches = math.ceil( len(item_ids) / float(DENOM))\n",
    "# batch_ls = [str(tuple(batch)) for batch in np.array_split(item_ids, n_batches)]\n",
    "# batch_lst = [(idx, row) for idx, row in enumerate(batch_ls)]\n",
    "\n",
    "# process_full_batch(batch_lst[5], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[6], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[7], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[8], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[9], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[10], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[11], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[12], save_db=True, save_fe=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
