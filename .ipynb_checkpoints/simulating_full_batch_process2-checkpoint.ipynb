{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height has been deprecated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from time import gmtime, strftime\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "from sqlalchemy import ForeignKey, Table, Column, String, Integer, Float, Boolean, MetaData, select\n",
    "from joblib import Parallel, delayed\n",
    "from impala.dbapi import connect\n",
    "from impala.util import as_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenixdb\n",
    "import phoenixdb.cursor\n",
    "conn = connect(host='133.186.168.6', port=21050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'rfind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-1d437e725d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# conn = connect(host='133.186.168.6', port=21050)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# database_url = 'jdbc:phoenix:tcepop-mst002,tcepop-mst003,tcepop-mst001:2181:/hbase'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphoenixdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'133.186.168.7:2181:/hbase'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocommit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/phoenixdb/__init__.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(url, max_retries, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \"\"\"\n\u001b[1;32m     65\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAvaticaClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/phoenixdb/avatica.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Opening connection to %s:%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhostname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhostname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhttplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, port, strict, timeout, source_address)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_hostport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# This is stored as an instance variable to allow unittests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_get_hostport\u001b[0;34m(self, host, port)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_hostport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m']'\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# ipv6 addresses have [...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'rfind'"
     ]
    }
   ],
   "source": [
    "conn = connect(host='133.186.168.7', port=21050)\n",
    "# database_url = 'jdbc:phoenix:tcepop-mst002,tcepop-mst003,tcepop-mst001:2181:/hbase'\n",
    "conn = phoenixdb.connect('133.186.168.7:2181:/hbase', autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_engineered_bundle(df):\n",
    "    \n",
    "    def get_arr_in_cluster(df):\n",
    "\n",
    "        empty_lst = []\n",
    "        for name, group in df.groupby('label')['STOCK_AMOUNT']:\n",
    "            result_lst = np.sort(np.diff(group))[1:-1]\n",
    "            empty_lst = np.append(empty_lst, result_lst)\n",
    "        arr_in_cluster = -empty_lst[empty_lst < 0]\n",
    "        return arr_in_cluster\n",
    "    \n",
    "    # The number of unique stock_id\n",
    "\n",
    "    df = df.set_index(\"REG_DT\")\n",
    "    unique_stock_ids = df.STOCK_ID.unique()\n",
    "    n_unique_stock_id = len(unique_stock_ids) \n",
    "    \n",
    "    # select a single stock_id\n",
    "    tmp2 = list(df.groupby('STOCK_ID'))[0][1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The ratio of NA\n",
    "    tmp3 = tmp2.resample('1D').first()\n",
    "    \n",
    "    # The number of days\n",
    "    n_days = len(tmp3.ID)\n",
    "    \n",
    "    if n_days <=1:\n",
    "        return\n",
    "    \n",
    "    null_arr = pd.isnull(tmp3.ID).values\n",
    "    ratio_of_na = sum(null_arr) / float(n_days)\n",
    "    \n",
    "    \n",
    "    consecutive_lst = [ sum( 1 for _ in group ) for key, group in itertools.groupby( null_arr ) if key ]\n",
    "    \n",
    "    \n",
    "    # The max value of consecutive NAs \n",
    "    max_consecutive_na = max([0] + consecutive_lst)\n",
    "    \n",
    "\n",
    "    # The instances of consecutive NAs\n",
    "    n_consecutive_na = len(consecutive_lst)\n",
    "    \n",
    "    # Define a stock array\n",
    "    stock_arr = tmp3.STOCK_AMOUNT.values\n",
    "    \n",
    "    # The medain\n",
    "    median_v = np.nanmedian(stock_arr)\n",
    "    \n",
    "    # Std\n",
    "    std_v = np.nanstd(stock_arr)\n",
    "    \n",
    "    # max, min\n",
    "    max_v = np.nanmax(stock_arr)\n",
    "    min_v = np.nanmin(stock_arr)\n",
    "    \n",
    "    # The range between max and min\n",
    "    range_v = max_v - min_v\n",
    "    \n",
    "    stock_na_removed = stock_arr[~np.isnan(stock_arr)]\n",
    "    \n",
    "    consecutive_same_lst = [ sum( 1 for _ in group ) for key, group in itertools.groupby( stock_na_removed ) if key ]\n",
    "    \n",
    "    if len(consecutive_same_lst) == 0:\n",
    "        ratio_same_value = 0\n",
    "    else:\n",
    "        ratio_same_value = max(consecutive_same_lst) / float(n_days)\n",
    "    \n",
    "    \n",
    "    n_jumps = sum(np.diff(stock_na_removed) > 0)\n",
    "    max_drop = -min(np.diff(stock_na_removed))\n",
    "    \n",
    "    tmp3['STOCK_AMOUNT'] = tmp3.STOCK_AMOUNT.replace(np.nan, -1)\n",
    "    n_cluster, label = get_label_from_dbscan(tmp3)\n",
    "    \n",
    "    tmp3 = tmp3.assign(label=label)\n",
    "\n",
    "    arr_in_cluster = get_arr_in_cluster(tmp3)\n",
    "\n",
    "    if len(arr_in_cluster) > 0:\n",
    "\n",
    "        mean_in_cluster = np.nanmean(arr_in_cluster)\n",
    "        std_in_cluster = np.nanstd(arr_in_cluster)\n",
    "    else:\n",
    "        mean_in_cluster = 0\n",
    "        std_in_cluster = 0\n",
    "\n",
    "    \n",
    "    bundle = {\n",
    "        'item_id': df.ITEM_ID.values[0],\n",
    "        'stock_id': tmp3.STOCK_ID.values[0],\n",
    "        'n_unique_stock_id': n_unique_stock_id,\n",
    "        'n_days': n_days,\n",
    "        'ratio_of_na': ratio_of_na,\n",
    "        'max_consecutive_na': max_consecutive_na,\n",
    "        'n_consecutive_na': n_consecutive_na,\n",
    "        'median_v': median_v,\n",
    "        'std_v': std_v,\n",
    "        'max_v': max_v,\n",
    "        'ratio_drop': max_drop / max(float(max_v), 0.00001),\n",
    "        'min_v': min_v,\n",
    "        'range_v': range_v,\n",
    "        'ratio_same_value': ratio_same_value,\n",
    "        'n_jumps': n_jumps,\n",
    "        'max_drop': max_drop,\n",
    "        'n_cluster': n_cluster,\n",
    "        'mean_in_cluster': mean_in_cluster,\n",
    "        'std_in_cluster': std_in_cluster\n",
    "\n",
    "    }\n",
    "    \n",
    "    return bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Epopcon_db:\n",
    "    def __init__(self, local_access=True):\n",
    "        \n",
    "        if local_access:\n",
    "        \n",
    "            self.wspider_temp = create_engine(\"mysql://eums:eums00!q@115.90.182.250:11000/wspider_temp\", pool_size=20, pool_recycle=3600,\n",
    "                           connect_args={'connect_timeout': 1000000})\n",
    "            self.wspider = create_engine(\"mysql://wspider:wspider00!q@192.168.0.36:3306/wspider\", pool_size=20, pool_recycle=3600,\n",
    "                       connect_args={'connect_timeout': 1000000})\n",
    "        else:\n",
    "            \n",
    "            self.wspider_temp = create_engine(\"mysql://eums:eums00!q@192.168.0.50:3306/wspider_temp\", pool_size=20, pool_recycle=3600,\n",
    "                           connect_args={'connect_timeout': 1000000})\n",
    "            \n",
    "            self.wspider = create_engine(\"mysql://wspider:wspider00!q@133.186.143.65:3306/wspider\", pool_size=20, pool_recycle=3600,\n",
    "                           connect_args={'connect_timeout': 1000000})\n",
    "                \n",
    "        add_engine_pidguard(self.wspider_temp)\n",
    "        add_engine_pidguard(self.wspider)\n",
    "    def get_engine(self, production=False):\n",
    "        \n",
    "        if production:\n",
    "            return self.wspider\n",
    "        else:\n",
    "            return self.wspider_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epopcon_db = Epopcon_db()\n",
    "\n",
    "wspider_engine = epopcon_db.get_engine(production=True)\n",
    "wspider_temp_engine = epopcon_db.get_engine(production=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(target):\n",
    "    \n",
    "    # cluster inventory data points\n",
    "    n_cluster, label = get_label_from_dbscan(target, eps=0.15, min_samples=3)\n",
    "    target = target.assign(label=label) \n",
    "    target = target[['STOCK_AMOUNT', 'label', 'REG_DT']]\n",
    "    labels = target.label.unique()\n",
    "    \n",
    "    # resample to a daily scale\n",
    "    target = target.set_index('REG_DT')\n",
    "    target = target.resample('1D').first()\n",
    "    \n",
    "    # placeholding\n",
    "    target['STOCK_AMOUNT_imputed'] = target['STOCK_AMOUNT']\n",
    "\n",
    "    # interpolate data points based on cluster group\n",
    "    for label in labels:\n",
    "        idx = np.where(target.label.values == label)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        start_v = min(idx)\n",
    "        end_v = max(idx)\n",
    "        target.loc[start_v:end_v+1, 'STOCK_AMOUNT_imputed'] = target['STOCK_AMOUNT'][start_v:end_v+1].interpolate(method='from_derivatives')\n",
    "\n",
    "    # interpolate data points based on global data points\n",
    "    target['STOCK_AMOUNT_imputed'] = target['STOCK_AMOUNT'].interpolate(method='from_derivatives')\n",
    "    \n",
    "    # round STOCK_AMOUNT_imputed to make it cleaner\n",
    "    target['STOCK_AMOUNT_imputed'] = target.STOCK_AMOUNT_imputed.round()\n",
    "\n",
    "    # calculate sell amount \n",
    "    target['sell'] = np.append([0], np.negative(np.diff(target.STOCK_AMOUNT_imputed)))\n",
    "    target.loc[target['sell'].values < 0, 'sell'] = np.nan\n",
    "    target.sell.astype(float)\n",
    "    \n",
    "    # calculate z-score for thresholding\n",
    "    target['zscore'] = np.abs(target.sell - target.sell.mean() / max(0.0001, target.sell.std()))\n",
    "\n",
    "    # get rid of outliers \n",
    "    target.loc[target['zscore'] > 4, 'sell'] = np.nan\n",
    "    \n",
    "    # prepare matrix for data imputation using KNN based on dayofweek\n",
    "    target['weekday_name'] = target.index.dayofweek\n",
    "    X_incomplete = target[['sell', 'weekday_name']].values\n",
    "\n",
    "    # run KNN to calculate sell_impute (imputed version of sell amount)\n",
    "    try:\n",
    "        X_filled_knn = KNN(k=1, verbose=False).complete(X_incomplete)\n",
    "        target['sell_impute'] = X_filled_knn[:,0]\n",
    "    except:\n",
    "        target['sell_impute'] = target['sell']\n",
    "    \n",
    "    # placeholding\n",
    "    target['STOCK_AMOUNT_imputed_trimed'] = target['STOCK_AMOUNT_imputed']\n",
    "    \n",
    "    # get rid of jumpbs\n",
    "    cond = np.append([0], np.negative(np.diff(target.STOCK_AMOUNT_imputed))) < 0\n",
    "    target.loc[cond, 'STOCK_AMOUNT_imputed_trimed'] = np.nan\n",
    "\n",
    "    return target\n",
    "\n",
    "# TODO optimize parameters using ML\n",
    "\n",
    "def get_filtered_fg_df(feature_engineered_df):\n",
    "    static_item_ids = feature_engineered_df.item_id[(feature_engineered_df.std_in_cluster == 0.0)].values\n",
    "    data_df_cleaned = feature_engineered_df[feature_engineered_df.mean_in_cluster.notnull()]\n",
    "    purified_df = data_df_cleaned[(data_df_cleaned.ratio_drop < 0.3)\n",
    "#                           & (data_df_cleaned.ratio_same_value < 0.3)\n",
    "                          & (data_df_cleaned.n_jumps <= 3)\n",
    "                          & (data_df_cleaned.n_days >= 3)\n",
    "#                           & (data_df_cleaned.std_in_cluster > 0.2)\n",
    "                          & (data_df_cleaned.std_in_cluster < 4)\n",
    "                          & (data_df_cleaned.ratio_of_na < 0.5)\n",
    "#                           & (data_df_cleaned.n_unique_stock_id < 50)\n",
    "                                 ]\n",
    "    return purified_df, static_item_ids\n",
    "\n",
    "def get_sell_amount_by_item_id(df, add_sell_amount=False):\n",
    "    \n",
    "    collect_day = df.COLLECT_DAY.values[0]\n",
    "    reg_id = df.REG_ID.values[0]\n",
    "    \n",
    "    imputed_df_lst = []\n",
    "    for stock_id, group_df in list(df.groupby('STOCK_ID')):\n",
    "        \n",
    "        imputed_df = impute_data(group_df)[['sell_impute', 'STOCK_AMOUNT', 'STOCK_AMOUNT_imputed_trimed']]\n",
    "        imputed_df['STOCK_ID'] = stock_id        \n",
    "        imputed_df_lst.append(imputed_df)\n",
    "        \n",
    "    imputed_df = pd.concat(imputed_df_lst)\n",
    "    imputed_df.columns = ['SELL_AMOUNT', 'STOCK_AMOUNT', 'REVISE_STOCK_AMOUNT', 'STOCK_ID']\n",
    "    imputed_df['ITEM_ID'] = df.ITEM_ID.values[0]\n",
    "    imputed_df['REG_ID'] = reg_id\n",
    "    imputed_df['UPT_DT'] = pd.to_datetime(datetime.now(timezone('Asia/Seoul')).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    imputed_df['COLLECT_DAY'] = collect_day\n",
    "    imputed_df['UPT_ID'] = 'FILTER ALGO'\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "def insert_extracted_feature(extracted_feature_df):\n",
    "    extracted_feature_df = extracted_feature_df.where((pd.notnull(extracted_feature_df)), None)\n",
    "    query = \"\"\"REPLACE MWS_COLT_ITEM_EXTRACTED_FEATURE %s VALUES %s \"\"\" % (tuple(extracted_feature_df.columns), tuple(['%s' for _ in range(len(extracted_feature_df.columns))]))\n",
    "    query = query.replace(\"'\", \"\")\n",
    "    wspider_temp_engine.execute(query, [tuple(x) for x in extracted_feature_df.values])\n",
    "\n",
    "def insert_sell_amt(sell_amt_df):\n",
    "    sell_amt_df = sell_amt_df.where((pd.notnull(sell_amt_df)), None)\n",
    "    query = \"\"\"REPLACE INTO MWS_COLT_ITEM_SELL_AMT_DEV %s VALUES %s \"\"\" % (tuple(sell_amt_df.columns), tuple(['%s' for _ in range(len(sell_amt_df.columns))]))\n",
    "    query = query.replace(\"'\", \"\")\n",
    "    wspider_temp_engine.execute(query, [tuple(x) for x in sell_amt_df.values])\n",
    "    \n",
    "    query2 = \"\"\"REPLACE INTO MWS_COLT_ITEM_SELL_AMT %s VALUES %s \"\"\" % (tuple(sell_amt_df.columns), tuple(['%s' for _ in range(len(sell_amt_df.columns))]))\n",
    "    query2 = query2.replace(\"'\", \"\")\n",
    "    wspider_engine.execute(query2, [tuple(x) for x in sell_amt_df.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(host='133.186.168.6', port=21050)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_full_batch(batches, save_db=True, save_img=False, save_fe=True):\n",
    "\n",
    "    # select multiple items\n",
    "    idx, query = batches\n",
    "#     batch = pd.read_sql_query(\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE ITEM_ID in %s\" % query, wspider_engine)\n",
    "\n",
    "\n",
    "    stmt = \"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN %s\" % query\n",
    "\n",
    "    cursor.execute(stmt)\n",
    "    batch = as_pandas(cursor)\n",
    "    batch.columns = [item.upper() for item in batch.columns]\n",
    "    batch['REG_DT'] = pd.to_datetime(batch['REG_DT'])\n",
    "    batch[['STOCK_AMOUNT']] = batch[['STOCK_AMOUNT']].astype(np.int)\n",
    "\n",
    "    # extract features by stock id\n",
    "    result_lst = []\n",
    "    for idx, group in batch.groupby('ITEM_ID'):\n",
    "#         tmp = list(group_by_item_id.groupby('STOCK_ID'))[0][1]    \n",
    "        result_lst.append(get_feature_engineered_bundle(group))\n",
    "\n",
    "    # clean up extracted feature df\n",
    "    extracted_feature_df = pd.DataFrame([result for result in result_lst if result != None])\n",
    "\n",
    "\n",
    "    try:\n",
    "        # filter dataframe based on extraction criteria\n",
    "        filtered_df, static_item_ids = get_filtered_fg_df(extracted_feature_df)\n",
    "\n",
    "        # filtered df\n",
    "        cleaned_item_ids = filtered_df.item_id.values\n",
    "        cleaned_df = batch[batch['ITEM_ID'].isin(cleaned_item_ids)]\n",
    "\n",
    "        # label extracted feature df\n",
    "        extracted_feature_df['condition_clean'] = 0\n",
    "        extracted_feature_df.loc[extracted_feature_df.item_id.isin(cleaned_item_ids), 'condition_clean'] = 1\n",
    "        extracted_feature_df.loc[extracted_feature_df.item_id.isin(static_item_ids), 'condition_clean'] = 2\n",
    "\n",
    "\n",
    "    except:\n",
    "        return\n",
    "\n",
    "    # save images\n",
    "    if save_img:\n",
    "        save_img(cleaned_df)\n",
    "\n",
    "    # save extracted features to db\n",
    "    if save_fe:\n",
    "        pass\n",
    "#         insert_extracted_feature(extracted_feature_df)\n",
    "\n",
    "\n",
    "    if save_db:\n",
    "\n",
    "        df_lst =[]\n",
    "        cleaned_df = cleaned_df.sort_values(by=['ITEM_ID', 'STOCK_ID', 'REG_DT'])\n",
    "        for idx, group in cleaned_df.groupby('ITEM_ID'):\n",
    "            try:\n",
    "                df_lst.append(get_sell_amount_by_item_id(group))\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "        if len(df_lst) > 0:\n",
    "\n",
    "            result = pd.concat(df_lst)\n",
    "            result[['COLLECT_DAY']] = result.index\n",
    "#             insert_sell_amt(result)\n",
    "#             result.to_sql(con=wspider_temp_engine, name='MWS_COLT_ITEM_SELL_AMT_DEV', if_exists='append')\n",
    "#             logging.warning('done with %s' % str(file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_df = pd.read_sql_query(\"SELECT ID FROM MWS_COLT_ITEM WHERE RELEASE_DT > '2018-01-01'\", wspider_engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENOM = 1000\n",
    "# item_ids = ids_df.ID.values\n",
    "item_ids = ids_df.ID.values.astype(str)\n",
    "n_batches = math.ceil( len(item_ids) / float(DENOM))\n",
    "batch_ls = [str(tuple(batch)) for batch in np.array_split(item_ids, n_batches)]\n",
    "batch_lst = [(idx, row) for idx, row in enumerate(batch_ls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1616"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.821527777777778"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(batch_lst) * 280) / 3600.0 / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 34s, sys: 71.9 ms, total: 4min 34s\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_full_batch(batch_lst[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.6 ms, sys: 16.7 ms, total: 91.2 ms\n",
      "Wall time: 6.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cursor = conn.cursor()\n",
    "\n",
    "idx, query = batch_lst[0]\n",
    "#     batch = pd.read_sql_query(\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE ITEM_ID in %s\" % query, wspider_engine)\n",
    "\n",
    "\n",
    "stmt = \"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN %s\" % query\n",
    "\n",
    "cursor.execute(stmt)\n",
    "batch = as_pandas(cursor)\n",
    "batch.columns = [item.upper() for item in batch.columns]\n",
    "batch['REG_DT'] = pd.to_datetime(batch['REG_DT'])\n",
    "batch[['STOCK_AMOUNT']] = batch[['STOCK_AMOUNT']].astype(np.int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 0 ns, total: 1.12 s\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# extract features by stock id\n",
    "result_lst = []\n",
    "for idx, group in batch.groupby('ITEM_ID'):\n",
    "#         tmp = list(group_by_item_id.groupby('STOCK_ID'))[0][1]    \n",
    "    result_lst.append(get_feature_engineered_bundle(group))\n",
    "\n",
    "# clean up extracted feature df\n",
    "extracted_feature_df = pd.DataFrame([result for result in result_lst if result != None])\n",
    "\n",
    "\n",
    "try:\n",
    "    # filter dataframe based on extraction criteria\n",
    "    filtered_df, static_item_ids = get_filtered_fg_df(extracted_feature_df)\n",
    "\n",
    "    # filtered df\n",
    "    cleaned_item_ids = filtered_df.item_id.values\n",
    "    cleaned_df = batch[batch['ITEM_ID'].isin(cleaned_item_ids)]\n",
    "\n",
    "    # label extracted feature df\n",
    "    extracted_feature_df['condition_clean'] = 0\n",
    "    extracted_feature_df.loc[extracted_feature_df.item_id.isin(cleaned_item_ids), 'condition_clean'] = 1\n",
    "    extracted_feature_df.loc[extracted_feature_df.item_id.isin(static_item_ids), 'condition_clean'] = 2\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sell_amount_by_item_id(df, add_sell_amount=False):\n",
    "    \n",
    "    collect_day = df.COLLECT_DAY.values[0]\n",
    "    reg_id = df.REG_ID.values[0]\n",
    "    stock_id = df.STOCK_ID.values[0]\n",
    "    \n",
    "#     imputed_df_lst = []\n",
    "#     for stock_id, group_df in df.groupby('STOCK_ID'):\n",
    "        \n",
    "    imputed_df = impute_data(df)[['sell_impute', 'STOCK_AMOUNT', 'STOCK_AMOUNT_imputed_trimed']]\n",
    "    imputed_df['STOCK_ID'] = stock_id        \n",
    "#     imputed_df_lst.append(imputed_df)\n",
    "        \n",
    "#     imputed_df = pd.concat(imputed_df_lst)\n",
    "    imputed_df.columns = ['SELL_AMOUNT', 'STOCK_AMOUNT', 'REVISE_STOCK_AMOUNT', 'STOCK_ID']\n",
    "    imputed_df['ITEM_ID'] = df.ITEM_ID.values[0]\n",
    "    imputed_df['REG_ID'] = reg_id\n",
    "    imputed_df['UPT_DT'] = pd.to_datetime(datetime.now(timezone('Asia/Seoul')).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    imputed_df['COLLECT_DAY'] = collect_day\n",
    "    imputed_df['UPT_ID'] = 'FILTER ALGO'\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(target):\n",
    "\n",
    "    stage1 = time.time()\n",
    "    print('first')\n",
    "    # cluster inventory data points\n",
    "    n_cluster, label = get_label_from_dbscan(target, eps=0.15, min_samples=3)\n",
    "    target = target.assign(label=label) \n",
    "    target = target[['STOCK_AMOUNT', 'label', 'REG_DT']]\n",
    "    labels = target.label.unique()\n",
    "    \n",
    "    # resample to a daily scale\n",
    "    target = target.set_index('REG_DT')\n",
    "    target = target.resample('1D').first()\n",
    "    \n",
    "    # placeholding\n",
    "    target['STOCK_AMOUNT_imputed'] = target['STOCK_AMOUNT']\n",
    "\n",
    "    \n",
    "    stage2 = time.time()\n",
    "    print(stage2 - stage1)\n",
    "    print('second')\n",
    "    \n",
    "    \n",
    "    # interpolate data points based on cluster group\n",
    "    for label in labels:\n",
    "        idx = np.where(target.label.values == label)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        start_v = min(idx)\n",
    "        end_v = max(idx)\n",
    "        target.loc[start_v:end_v+1, 'STOCK_AMOUNT_imputed'] = target['STOCK_AMOUNT'][start_v:end_v+1].interpolate(method='from_derivatives')\n",
    "\n",
    "    # interpolate data points based on global data points\n",
    "    target['STOCK_AMOUNT_imputed'] = target['STOCK_AMOUNT'].interpolate(method='from_derivatives')\n",
    "    \n",
    "    # round STOCK_AMOUNT_imputed to make it cleaner\n",
    "    target['STOCK_AMOUNT_imputed'] = target.STOCK_AMOUNT_imputed.round()\n",
    "\n",
    "    # calculate sell amount \n",
    "    target['sell'] = np.append([0], np.negative(np.diff(target.STOCK_AMOUNT_imputed)))\n",
    "    target.loc[target['sell'].values < 0, 'sell'] = np.nan\n",
    "    target.sell.astype(float)\n",
    "\n",
    "    stage3 = time.time()\n",
    "    print(stage3 - stage2)\n",
    "    print('third')\n",
    "    \n",
    "    # calculate z-score for thresholding\n",
    "    target['zscore'] = np.abs(target.sell - target.sell.mean() / max(0.0001, target.sell.std()))\n",
    "    \n",
    "    # get rid of outliers \n",
    "    target.loc[target['zscore'] > 4, 'sell'] = np.nan\n",
    "    \n",
    "    # prepare matrix for data imputation using KNN based on dayofweek\n",
    "    target['weekday_name'] = target.index.dayofweek\n",
    "    X_incomplete = target[['sell', 'weekday_name']].values\n",
    "\n",
    "    # run KNN to calculate sell_impute (imputed version of sell amount)\n",
    "    try:\n",
    "        X_filled_knn = KNN(k=1).complete(X_incomplete)\n",
    "        target['sell_impute'] = X_filled_knn[:,0]\n",
    "    except:\n",
    "        target['sell_impute'] = target['sell']\n",
    "    \n",
    "    # placeholding\n",
    "    target['STOCK_AMOUNT_imputed_trimed'] = target['STOCK_AMOUNT_imputed']\n",
    "    \n",
    "    # get rid of jumpbs\n",
    "    cond = np.append([0], np.negative(np.diff(target.STOCK_AMOUNT_imputed))) < 0\n",
    "    target.loc[cond, 'STOCK_AMOUNT_imputed_trimed'] = np.nan\n",
    "\n",
    "#     print(stage2 - stage1)\n",
    "#     print(stage3 - stage2)    \n",
    "    \n",
    "    stage4 = time.time()\n",
    "    print(stage4 - stage3)\n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first\n",
      "0.00849294662476\n",
      "second\n",
      "0.00529217720032\n",
      "third\n",
      "0.00668978691101\n",
      "first\n",
      "0.00773000717163\n",
      "second\n",
      "0.00517821311951\n",
      "third\n",
      "0.00652194023132\n",
      "first\n",
      "0.00769782066345\n",
      "second\n",
      "0.0051851272583\n",
      "third\n",
      "0.00693202018738\n",
      "first\n",
      "0.00855398178101\n",
      "second\n",
      "0.00597405433655\n",
      "third\n",
      "0.00682687759399\n",
      "first\n",
      "0.00851583480835\n",
      "second\n",
      "0.00556421279907\n",
      "third\n",
      "0.00691294670105\n",
      "first\n",
      "0.00800704956055\n",
      "second\n",
      "0.00539803504944\n",
      "third\n",
      "0.00675201416016\n",
      "first\n",
      "0.0079619884491\n",
      "second\n",
      "0.00525808334351\n",
      "third\n",
      "0.00660181045532\n",
      "first\n",
      "0.00773501396179\n",
      "second\n",
      "0.00518012046814\n",
      "third\n",
      "0.00686693191528\n",
      "first\n",
      "0.00817584991455\n",
      "second\n",
      "0.00542402267456\n",
      "third\n",
      "0.00685715675354\n",
      "first\n",
      "0.00772404670715\n",
      "second\n",
      "0.00523495674133\n",
      "third\n",
      "0.00655102729797\n",
      "first\n",
      "0.00773096084595\n",
      "second\n",
      "0.00517606735229\n",
      "third\n",
      "0.00648093223572\n",
      "first\n",
      "0.00778603553772\n",
      "second\n",
      "0.0051748752594\n",
      "third\n",
      "0.00650811195374\n",
      "first\n",
      "0.0081570148468\n",
      "second\n",
      "0.00540614128113\n",
      "third\n",
      "0.00670289993286\n",
      "first\n",
      "0.00813007354736\n",
      "second\n",
      "0.00534510612488\n",
      "third\n",
      "0.00682187080383\n",
      "first\n",
      "0.00804591178894\n",
      "second\n",
      "0.00532793998718\n",
      "third\n",
      "0.00662302970886\n",
      "first\n",
      "0.00833582878113\n",
      "second\n",
      "0.00542998313904\n",
      "third\n",
      "0.00676012039185\n",
      "first\n",
      "0.00815987586975\n",
      "second\n",
      "0.00533819198608\n",
      "third\n",
      "0.00702881813049\n",
      "first\n",
      "0.00819516181946\n",
      "second\n",
      "0.0053768157959\n",
      "third\n",
      "0.00678014755249\n",
      "first\n",
      "0.00809502601624\n",
      "second\n",
      "0.00537300109863\n",
      "third\n",
      "0.00676798820496\n",
      "first\n",
      "0.00815606117249\n",
      "second\n",
      "0.00539588928223\n",
      "third\n",
      "0.00668215751648\n",
      "first\n",
      "0.00807690620422\n",
      "second\n",
      "0.00531697273254\n",
      "third\n",
      "0.00669312477112\n",
      "first\n",
      "0.00823497772217\n",
      "second\n",
      "0.00536298751831\n",
      "third\n",
      "0.00671195983887\n",
      "first\n",
      "0.00839805603027\n",
      "second\n",
      "0.00556898117065\n",
      "third\n",
      "0.00691485404968\n",
      "first\n",
      "0.00896286964417\n",
      "second\n",
      "0.00598502159119\n",
      "third\n",
      "0.00720810890198\n",
      "first\n",
      "0.00816202163696\n",
      "second\n",
      "0.00541186332703\n",
      "third\n",
      "0.00676798820496\n",
      "first\n",
      "0.0080361366272\n",
      "second\n",
      "0.00539183616638\n",
      "third\n",
      "0.00677895545959\n",
      "first\n",
      "0.00810408592224\n",
      "second\n",
      "0.00538396835327\n",
      "third\n",
      "0.00679898262024\n",
      "first\n",
      "0.00851488113403\n",
      "second\n",
      "0.00569200515747\n",
      "third\n",
      "0.00719809532166\n",
      "first\n",
      "0.00849103927612\n",
      "second\n",
      "0.0056848526001\n",
      "third\n",
      "0.00715303421021\n",
      "first\n",
      "0.00802183151245\n",
      "second\n",
      "0.00535202026367\n",
      "third\n",
      "0.00701212882996\n",
      "first\n",
      "0.0080029964447\n",
      "second\n",
      "0.00539803504944\n",
      "third\n",
      "0.00700902938843\n",
      "first\n",
      "0.00801801681519\n",
      "second\n",
      "0.00570106506348\n",
      "third\n",
      "0.00680208206177\n",
      "first\n",
      "0.00888800621033\n",
      "second\n",
      "0.00547313690186\n",
      "third\n",
      "0.00672793388367\n",
      "first\n",
      "0.00788712501526\n",
      "second\n",
      "0.00585389137268\n",
      "third\n",
      "0.00771617889404\n",
      "first\n",
      "0.00990891456604\n",
      "second\n",
      "0.0060760974884\n",
      "third\n",
      "0.0076789855957\n",
      "first\n",
      "0.00831508636475\n",
      "second\n",
      "0.00602412223816\n",
      "third\n",
      "0.00708699226379\n",
      "first\n",
      "0.00906896591187\n",
      "second\n",
      "0.0063910484314\n",
      "third\n",
      "0.00731301307678\n",
      "first\n",
      "0.00792288780212\n",
      "second\n",
      "0.00574707984924\n",
      "third\n",
      "0.00762701034546\n",
      "first\n",
      "0.00859689712524\n",
      "second\n",
      "0.00557994842529\n",
      "third\n",
      "0.00738215446472\n",
      "first\n",
      "0.0083270072937\n",
      "second\n",
      "0.00555491447449\n",
      "third\n",
      "0.00693011283875\n",
      "first\n",
      "0.00858783721924\n",
      "second\n",
      "0.00552201271057\n",
      "third\n",
      "0.0065929889679\n",
      "first\n",
      "0.0083019733429\n",
      "second\n",
      "0.00588202476501\n",
      "third\n",
      "0.00660109519958\n",
      "first\n",
      "0.00970196723938\n",
      "second\n",
      "0.00585985183716\n",
      "third\n",
      "0.00675415992737\n",
      "first\n",
      "0.00800395011902\n",
      "second\n",
      "0.00662899017334\n",
      "third\n",
      "0.00785303115845\n",
      "first\n",
      "0.00886511802673\n",
      "second\n",
      "0.00579881668091\n",
      "third\n",
      "0.00678610801697\n",
      "first\n",
      "0.00836992263794\n",
      "second\n",
      "0.00589990615845\n",
      "third\n",
      "0.0068941116333\n",
      "first\n",
      "0.00812005996704\n",
      "second\n",
      "0.00585794448853\n",
      "third\n",
      "0.00770616531372\n",
      "first\n",
      "0.0081250667572\n",
      "second\n",
      "0.00606513023376\n",
      "third\n",
      "0.00728988647461\n",
      "first\n",
      "0.00915193557739\n",
      "second\n",
      "0.00628590583801\n",
      "third\n",
      "0.00736308097839\n",
      "first\n",
      "0.0080931186676\n",
      "second\n",
      "0.00588893890381\n",
      "third\n",
      "0.00724697113037\n",
      "first\n",
      "0.00804400444031\n",
      "second\n",
      "0.00645685195923\n",
      "third\n",
      "0.00837516784668\n",
      "CPU times: user 7.13 s, sys: 88.1 ms, total: 7.22 s\n",
      "Wall time: 7.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_lst =[]\n",
    "cleaned_df = cleaned_df.sort_values(by=['ITEM_ID', 'STOCK_ID', 'REG_DT'])\n",
    "# for idx, group in cleaned_df.groupby('ITEM_ID'):\n",
    "#     try:\n",
    "#         df_lst.append(get_sell_amount_by_item_id(group))\n",
    "\n",
    "#     except:\n",
    "#         continue\n",
    "\n",
    "for idx, group in cleaned_df.groupby('STOCK_ID'):\n",
    "    df_lst.append(get_sell_amount_by_item_id(group))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = list(cleaned_df.groupby('ITEM_ID'))[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(cleaned_df.groupby('ITEM_ID')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.129281997681\n",
      "0.00507307052612\n",
      "0.116492986679\n",
      "0.00487995147705\n",
      "0.107362985611\n",
      "0.00487804412842\n",
      "0.11092209816\n",
      "0.00557899475098\n",
      "0.129693031311\n",
      "0.00490808486938\n",
      "0.107007026672\n",
      "0.00496602058411\n",
      "0.104826927185\n",
      "0.00517106056213\n",
      "0.108034133911\n",
      "0.00493383407593\n",
      "0.107087850571\n",
      "0.0048611164093\n",
      "0.105520963669\n",
      "0.00487208366394\n",
      "0.106205940247\n",
      "0.00552105903625\n",
      "0.104516983032\n",
      "0.00493001937866\n",
      "0.105206012726\n",
      "0.00486397743225\n",
      "0.108323097229\n",
      "0.00499391555786\n",
      "0.123515129089\n",
      "0.00517201423645\n",
      "0.106996059418\n",
      "0.00570893287659\n",
      "0.109642028809\n",
      "0.00550699234009\n",
      "0.15710401535\n",
      "0.00501799583435\n",
      "0.107882022858\n",
      "0.00501108169556\n",
      "0.109417200089\n",
      "0.00551080703735\n",
      "0.160938024521\n",
      "0.00561380386353\n",
      "0.160723924637\n",
      "0.00522685050964\n",
      "0.162782907486\n",
      "0.00561714172363\n",
      "0.159893035889\n",
      "0.00505208969116\n",
      "0.102391958237\n",
      "0.00483918190002\n",
      "0.107743024826\n",
      "0.00488305091858\n",
      "0.12407708168\n",
      "0.00556993484497\n",
      "0.158871173859\n",
      "0.00521802902222\n",
      "0.107818126678\n",
      "0.00484895706177\n",
      "0.116626024246\n",
      "0.00500798225403\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SELL_AMOUNT</th>\n",
       "      <th>STOCK_AMOUNT</th>\n",
       "      <th>REVISE_STOCK_AMOUNT</th>\n",
       "      <th>STOCK_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>REG_ID</th>\n",
       "      <th>UPT_DT</th>\n",
       "      <th>COLLECT_DAY</th>\n",
       "      <th>UPT_ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REG_DT</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578001</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578001</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578001</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578001</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578001</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-02</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578030</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-03</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578030</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-04</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578030</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578030</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-06</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>28975578030</td>\n",
       "      <td>6739767</td>\n",
       "      <td>SERVER</td>\n",
       "      <td>2018-03-07 10:45:01</td>\n",
       "      <td>20180102</td>\n",
       "      <td>FILTER ALGO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            SELL_AMOUNT  STOCK_AMOUNT  REVISE_STOCK_AMOUNT     STOCK_ID  ITEM_ID  REG_ID              UPT_DT COLLECT_DAY       UPT_ID\n",
       "REG_DT                                                                                                                               \n",
       "2018-01-02          0.0         999.0                999.0  28975578001  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "2018-01-03         -0.0           NaN                999.0  28975578001  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "2018-01-04         -0.0           NaN                999.0  28975578001  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "2018-01-05         -0.0           NaN                999.0  28975578001  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "2018-01-06         -0.0         999.0                999.0  28975578001  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "...                 ...           ...                  ...          ...      ...     ...                 ...         ...          ...\n",
       "2018-03-02         -0.0         999.0                999.0  28975578030  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "2018-03-03         -0.0         999.0                999.0  28975578030  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "2018-03-04         -0.0         999.0                999.0  28975578030  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "2018-03-05         -0.0         999.0                999.0  28975578030  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "2018-03-06         -0.0         999.0                999.0  28975578030  6739767  SERVER 2018-03-07 10:45:01    20180102  FILTER ALGO\n",
       "\n",
       "[1920 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sell_amount_by_item_id(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    dic['name'] = 'Dan1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Dan1'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_lst[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibHiveServer2Error",
     "evalue": "JoblibHiveServer2Error\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/opt/conda/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    169     pkg_name = mod_name.rpartition('.')[0]\n    170     main_globals = sys.modules[\"__main__\"].__dict__\n    171     if alter_argv:\n    172         sys.argv[0] = fname\n    173     return _run_code(code, main_globals, None,\n--> 174                      \"__main__\", fname, loader, pkg_name)\n        fname = '/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = ''\n    175 \n    176 def run_module(mod_name, init_globals=None,\n    177                run_name=None, alter_sys=False):\n    178     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/opt/conda/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7fb81a3e0930, file \"/...2.7/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': '', 'app': <module 'ipykernel.kernelapp' from '/opt/conda/lib/python2.7/site-packages/ipykernel/kernelapp.pyc'>, 'sys': <module 'sys' (built-in)>}, init_globals=None, mod_name='__main__', mod_fname='/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7fb81a3e0930, file \"/...2.7/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': '', 'app': <module 'ipykernel.kernelapp' from '/opt/conda/lib/python2.7/site-packages/ipykernel/kernelapp.pyc'>, 'sys': <module 'sys' (built-in)>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {u'date': datetime.datetime(2018, 3, 6, 16, 19, 46, 572356, tzinfo=tzlocal()), u'msg_id': u'ac536491a5c0e203cb886392d4c49f83', u'msg_type': u'execute_request', u'session': u'6aa599c0d2b844ab82e36f470d94e6af', u'username': u'', u'version': u'5.2'}, 'metadata': {}, 'msg_id': u'ac536491a5c0e203cb886392d4c49f83', 'msg_type': u'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['6aa599c0d2b844ab82e36f470d94e6af']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {u'date': datetime.datetime(2018, 3, 6, 16, 19, 46, 572356, tzinfo=tzlocal()), u'msg_id': u'ac536491a5c0e203cb886392d4c49f83', u'msg_type': u'execute_request', u'session': u'6aa599c0d2b844ab82e36f470d94e6af', u'username': u'', u'version': u'5.2'}, 'metadata': {}, 'msg_id': u'ac536491a5c0e203cb886392d4c49f83', 'msg_type': u'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['6aa599c0d2b844ab82e36f470d94e6af'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {u'date': datetime.datetime(2018, 3, 6, 16, 19, 46, 572356, tzinfo=tzlocal()), u'msg_id': u'ac536491a5c0e203cb886392d4c49f83', u'msg_type': u'execute_request', u'session': u'6aa599c0d2b844ab82e36f470d94e6af', u'username': u'', u'version': u'5.2'}, 'metadata': {}, 'msg_id': u'ac536491a5c0e203cb886392d4c49f83', 'msg_type': u'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', store_history=True, silent=False, shell_futures=True)\n   2713                 self.displayhook.exec_result = result\n   2714 \n   2715                 # Execute the user code\n   2716                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2717                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2718                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2719                 \n   2720                 self.last_execution_succeeded = not has_raised\n   2721 \n   2722                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-43-4e73910a79a6>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<ExecutionResult object at 7fb79ce104d0, executi..._before_exec=None error_in_exec=None result=None>)\n   2817 \n   2818         try:\n   2819             for i, node in enumerate(to_run_exec):\n   2820                 mod = ast.Module([node])\n   2821                 code = compiler(mod, cell_name, \"exec\")\n-> 2822                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fb79f7247b0, file \"<ipython-input-43-4e73910a79a6>\", line 4>\n        result = <ExecutionResult object at 7fb79ce104d0, executi..._before_exec=None error_in_exec=None result=None>\n   2823                     return True\n   2824 \n   2825             for i, node in enumerate(to_run_interactive):\n   2826                 mod = ast.Interactive([node])\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fb79f7247b0, file \"<ipython-input-43-4e73910a79a6>\", line 4>, result=<ExecutionResult object at 7fb79ce104d0, executi..._before_exec=None error_in_exec=None result=None>)\n   2877         outflag = 1  # happens in more places, so it's easier as default\n   2878         try:\n   2879             try:\n   2880                 self.hooks.pre_run_code_hook()\n   2881                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2882                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fb79f7247b0, file \"<ipython-input-43-4e73910a79a6>\", line 4>\n        self.user_global_ns = {'Boolean': <class 'sqlalchemy.sql.sqltypes.Boolean'>, 'ClientError': <class 'botocore.exceptions.ClientError'>, 'Column': <class 'sqlalchemy.sql.schema.Column'>, 'DBSCAN': <class 'sklearn.cluster.dbscan_.DBSCAN'>, 'DENOM': 10000, 'Epopcon_db': <class __main__.Epopcon_db>, 'Float': <class 'sqlalchemy.sql.sqltypes.Float'>, 'ForeignKey': <class 'sqlalchemy.sql.schema.ForeignKey'>, 'In': ['', u'from util import *\\nfrom time import gmtime, s...mport connect\\nfrom impala.util import as_pandas', u'class Epopcon_db:\\n    def __init__(self, loca...     else:\\n            return self.wspider_temp', u'epopcon_db = Epopcon_db()\\n\\nwspider_engine = ...engine = epopcon_db.get_engine(production=False)', u'def impute_data(target):\\n    \\n    # cluster ...(query2, [tuple(x) for x in sell_amt_df.values])', u\"conn = connect(host='133.186.168.6', port=21050)\\ncursor = conn.cursor()\", u'\\n\\ndef process_full_batch(batches, save_db=Tr...   logging.warning(\\'done with %s\\' % str(file))', u'ids_df = pd.read_sql_query(\"SELECT ID FROM MWS...RE RELEASE_DT > \\'2018-01-01\\'\", wspider_engine)', u'DENOM = 100\\n# item_ids = ids_df.ID.values\\nit...[(idx, row) for idx, row in enumerate(batch_ls)]', u'len(batch_lst)', u'from joblib import Parallel, delayed', u'# select multiple items\\nidx, query = batch_ls...T\\']] = batch[[\\'STOCK_AMOUNT\\']].astype(np.int)', u\"start_time = time.time()\\n\\n# def process_full...ed_time = time.time() - start_time\\nelapsed_time\", u'4.5 * 16049', u'4.5 * 16049 / 3600', u'(4.5 * 16049 / 3600) / 16', u'DENOM = 100000\\n# item_ids = ids_df.ID.values\\...[(idx, row) for idx, row in enumerate(batch_ls)]', u'len(batch_lst)', u'from joblib import Parallel, delayed', u'start_time = time.time()\\nprocess_full_batch(batch_lst[1])', ...], 'Integer': <class 'sqlalchemy.sql.sqltypes.Integer'>, ...}\n        self.user_ns = {'Boolean': <class 'sqlalchemy.sql.sqltypes.Boolean'>, 'ClientError': <class 'botocore.exceptions.ClientError'>, 'Column': <class 'sqlalchemy.sql.schema.Column'>, 'DBSCAN': <class 'sklearn.cluster.dbscan_.DBSCAN'>, 'DENOM': 10000, 'Epopcon_db': <class __main__.Epopcon_db>, 'Float': <class 'sqlalchemy.sql.sqltypes.Float'>, 'ForeignKey': <class 'sqlalchemy.sql.schema.ForeignKey'>, 'In': ['', u'from util import *\\nfrom time import gmtime, s...mport connect\\nfrom impala.util import as_pandas', u'class Epopcon_db:\\n    def __init__(self, loca...     else:\\n            return self.wspider_temp', u'epopcon_db = Epopcon_db()\\n\\nwspider_engine = ...engine = epopcon_db.get_engine(production=False)', u'def impute_data(target):\\n    \\n    # cluster ...(query2, [tuple(x) for x in sell_amt_df.values])', u\"conn = connect(host='133.186.168.6', port=21050)\\ncursor = conn.cursor()\", u'\\n\\ndef process_full_batch(batches, save_db=Tr...   logging.warning(\\'done with %s\\' % str(file))', u'ids_df = pd.read_sql_query(\"SELECT ID FROM MWS...RE RELEASE_DT > \\'2018-01-01\\'\", wspider_engine)', u'DENOM = 100\\n# item_ids = ids_df.ID.values\\nit...[(idx, row) for idx, row in enumerate(batch_ls)]', u'len(batch_lst)', u'from joblib import Parallel, delayed', u'# select multiple items\\nidx, query = batch_ls...T\\']] = batch[[\\'STOCK_AMOUNT\\']].astype(np.int)', u\"start_time = time.time()\\n\\n# def process_full...ed_time = time.time() - start_time\\nelapsed_time\", u'4.5 * 16049', u'4.5 * 16049 / 3600', u'(4.5 * 16049 / 3600) / 16', u'DENOM = 100000\\n# item_ids = ids_df.ID.values\\...[(idx, row) for idx, row in enumerate(batch_ls)]', u'len(batch_lst)', u'from joblib import Parallel, delayed', u'start_time = time.time()\\nprocess_full_batch(batch_lst[1])', ...], 'Integer': <class 'sqlalchemy.sql.sqltypes.Integer'>, ...}\n   2883             finally:\n   2884                 # Reset our crash handler in place\n   2885                 sys.excepthook = old_excepthook\n   2886         except SystemExit as e:\n\n...........................................................................\n/opt/notebooks/epopcon_lf/<ipython-input-43-4e73910a79a6> in <module>()\n      1 \n      2 \n      3 # select multiple items\n----> 4 start_time = time.time()\n      5 \n      6 Parallel(n_jobs=-1)(map(delayed(wrapper), batch_lst[:16]))\n      7     \n      8 time.time() - start_time\n      9 \n     10 \n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=[(<function wrapper>, ((0, \"('6739668', '6739669', '6739670', '6739671', '67...632', '6749633', '6749634', '6749635', '6749636')\"),), {}), (<function wrapper>, ((1, \"('6749637', '6749638', '6749639', '6749640', '67...601', '6759602', '6759603', '6759604', '6759605')\"),), {}), (<function wrapper>, ((2, \"('6759606', '6759607', '6759608', '6759609', '67...570', '6769571', '6769572', '6769573', '6769574')\"),), {}), (<function wrapper>, ((3, \"('6769575', '6769576', '6769577', '6769578', '67...539', '6779540', '6779541', '6779542', '6779543')\"),), {}), (<function wrapper>, ((4, \"('6779544', '6779545', '6779546', '6779547', '67...508', '6789509', '6789510', '6789511', '6789512')\"),), {}), (<function wrapper>, ((5, \"('6789513', '6789514', '6789515', '6789516', '67...477', '6799478', '6799479', '6799480', '6799481')\"),), {}), (<function wrapper>, ((6, \"('6799482', '6799483', '6799484', '6799485', '67...446', '6809447', '6809448', '6809449', '6809450')\"),), {}), (<function wrapper>, ((7, \"('6809451', '6809452', '6809453', '6809454', '68...415', '6819416', '6819417', '6819418', '6819419')\"),), {}), (<function wrapper>, ((8, \"('6819420', '6819421', '6819422', '6819423', '68...384', '6829385', '6829386', '6829387', '6829388')\"),), {}), (<function wrapper>, ((9, \"('6829389', '6829390', '6829391', '6829392', '68...353', '6839354', '6839355', '6839356', '6839357')\"),), {}), (<function wrapper>, ((10, \"('6839358', '6839359', '6839360', '6839361', '68...322', '6849323', '6849324', '6849325', '6849326')\"),), {}), (<function wrapper>, ((11, \"('6849327', '6849328', '6849329', '6849330', '68...291', '6859292', '6859293', '6859294', '6859295')\"),), {}), (<function wrapper>, ((12, \"('6859296', '6859297', '6859298', '6859299', '68...260', '6869261', '6869262', '6869263', '6869264')\"),), {}), (<function wrapper>, ((13, \"('6869265', '6869266', '6869267', '6869268', '68...229', '6879230', '6879231', '6879232', '6879233')\"),), {}), (<function wrapper>, ((14, \"('6879234', '6879235', '6879236', '6879237', '68...198', '6889199', '6889200', '6889201', '6889202')\"),), {}), (<function wrapper>, ((15, \"('6889203', '6889204', '6889205', '6889206', '68...167', '6899168', '6899169', '6899170', '6899171')\"),), {})])\n    807             if pre_dispatch == \"all\" or n_jobs == 1:\n    808                 # The iterable was consumed all at once by the above for loop.\n    809                 # No need to wait for async callbacks to trigger to\n    810                 # consumption.\n    811                 self._iterating = False\n--> 812             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    813             # Make sure that we get a last message telling us we are done\n    814             elapsed_time = time.time() - self._start_time\n    815             self._print('Done %3i out of %3i | elapsed: %s finished',\n    816                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nHiveServer2Error                                   Tue Mar  6 16:20:33 2018\nPID: 17843                             Python 2.7.14: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/joblib/parallel.pyc in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/opt/notebooks/epopcon_lf/<ipython-input-41-813bb8df29e3> in wrapper(batch=(5, \"('6789513', '6789514', '6789515', '6789516', '67...477', '6799478', '6799479', '6799480', '6799481')\"))\n      4     #     batch = pd.read_sql_query(\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE ITEM_ID in %s\" % query, wspider_engine)\n      5 \n      6 \n      7     stmt = \"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN %s\" % query\n      8 \n----> 9     cursor.execute(stmt)\n     10     batch = as_pandas(cursor)\n     11     batch.columns = [item.upper() for item in batch.columns]\n     12     batch['REG_DT'] = pd.to_datetime(batch['REG_DT'])\n     13     batch[['STOCK_AMOUNT']] = batch[['STOCK_AMOUNT']].astype(np.int)\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in execute(self=<impala.hiveserver2.HiveServer2Cursor object>, operation=\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN...477', '6799478', '6799479', '6799480', '6799481')\", parameters=None, configuration=None)\n    182             self._reset_state()\n    183 \n    184     def execute(self, operation, parameters=None, configuration=None):\n    185         # PEP 249\n    186         self.execute_async(operation, parameters=parameters,\n--> 187                            configuration=configuration)\n    188         log.info('Waiting for query to finish')\n    189         self._wait_to_finish()  # make execute synchronous\n    190         log.info('Query finished')\n    191 \n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in execute_async(self=<impala.hiveserver2.HiveServer2Cursor object>, operation=\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN...477', '6799478', '6799479', '6799480', '6799481')\", parameters=None, configuration=None)\n    200                 self._last_operation_string = operation\n    201             self._last_operation_handle = execute_statement(\n    202                 self.service, self.session_handle, self._last_operation_string,\n    203                 configuration)\n    204 \n--> 205         self._execute_async(op)\n    206 \n    207     def _debug_log_state(self):\n    208         log.debug(\n    209             '_execute_async: self._buffer=%s self._description=%s '\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in _execute_async(self=<impala.hiveserver2.HiveServer2Cursor object>, operation_fn=<function op>)\n    215         # operation_fn should set self._last_operation_string and\n    216         # self._last_operation_handle\n    217         self._debug_log_state()\n    218         self._reset_state()\n    219         self._debug_log_state()\n--> 220         operation_fn()\n    221         self._last_operation_active = True\n    222         self._debug_log_state()\n    223 \n    224     def _reset_state(self):\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in op()\n    198                                                                parameters)\n    199             else:\n    200                 self._last_operation_string = operation\n    201             self._last_operation_handle = execute_statement(\n    202                 self.service, self.session_handle, self._last_operation_string,\n--> 203                 configuration)\n    204 \n    205         self._execute_async(op)\n    206 \n    207     def _debug_log_state(self):\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in wrapper(*args=(<impala._thrift_gen.ImpalaService.ImpalaHiveServer2Service.Client object>, TSessionHandle(sessionId=THandleIdentifier(secre...d='bS3\\xb0\\xc1,@\\x83\\xb8\\xedt\\x80\\x17C\\xa6\\xb1')), \"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN...477', '6799478', '6799479', '6799480', '6799481')\", None), **kwargs={})\n    552                 if six.PY2 and not transport.isOpen():\n    553                     transport.open()\n    554                 elif six.PY3 and not transport.is_open():\n    555                     transport.open()\n    556                 log.debug('Transport opened')\n--> 557                 return func(*args, **kwargs)\n    558             except socket.error:\n    559                 log.exception('Failed to open transport (tries_left=%s)',\n    560                               tries_left)\n    561             except TTransportException:\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in execute_statement(service=<impala._thrift_gen.ImpalaService.ImpalaHiveServer2Service.Client object>, session_handle=TSessionHandle(sessionId=THandleIdentifier(secre...d='bS3\\xb0\\xc1,@\\x83\\xb8\\xedt\\x80\\x17C\\xa6\\xb1')), statement=\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN...477', '6799478', '6799479', '6799480', '6799481')\", configuration=None, async=False)\n    637                                statement=statement, confOverlay=configuration,\n    638                                runAsync=async)\n    639     log.debug('execute_statement: req=%s', req)\n    640     resp = service.ExecuteStatement(req)\n    641     log.debug('execute_statement: resp=%s', resp)\n--> 642     err_if_rpc_not_ok(resp)\n    643     return resp.operationHandle\n    644 \n    645 \n    646 @retry\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in err_if_rpc_not_ok(resp=TExecuteStatementResp(status=TStatus(errorCode=N...ssages=None, statusCode=3), operationHandle=None))\n    492 \n    493 def err_if_rpc_not_ok(resp):\n    494     if (resp.status.statusCode != TStatusCode.SUCCESS_STATUS and\n    495             resp.status.statusCode != TStatusCode.SUCCESS_WITH_INFO_STATUS and\n    496             resp.status.statusCode != TStatusCode.STILL_EXECUTING_STATUS):\n--> 497         raise HiveServer2Error(resp.status.errorMessage)\n    498 \n    499 \n    500 # datetime only supports 6 digits of microseconds but Impala supports 9.\n    501 # If present, the trailing 3 digits will be ignored without warning.\n\nHiveServer2Error: ArrayIndexOutOfBoundsException: 9\n\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mJoblibHiveServer2Error\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-4e73910a79a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m                         \u001b[0;31m# a working pool as they expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibHiveServer2Error\u001b[0m: JoblibHiveServer2Error\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/opt/conda/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    169     pkg_name = mod_name.rpartition('.')[0]\n    170     main_globals = sys.modules[\"__main__\"].__dict__\n    171     if alter_argv:\n    172         sys.argv[0] = fname\n    173     return _run_code(code, main_globals, None,\n--> 174                      \"__main__\", fname, loader, pkg_name)\n        fname = '/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = ''\n    175 \n    176 def run_module(mod_name, init_globals=None,\n    177                run_name=None, alter_sys=False):\n    178     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/opt/conda/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7fb81a3e0930, file \"/...2.7/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': '', 'app': <module 'ipykernel.kernelapp' from '/opt/conda/lib/python2.7/site-packages/ipykernel/kernelapp.pyc'>, 'sys': <module 'sys' (built-in)>}, init_globals=None, mod_name='__main__', mod_fname='/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7fb81a3e0930, file \"/...2.7/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': '', 'app': <module 'ipykernel.kernelapp' from '/opt/conda/lib/python2.7/site-packages/ipykernel/kernelapp.pyc'>, 'sys': <module 'sys' (built-in)>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {u'date': datetime.datetime(2018, 3, 6, 16, 19, 46, 572356, tzinfo=tzlocal()), u'msg_id': u'ac536491a5c0e203cb886392d4c49f83', u'msg_type': u'execute_request', u'session': u'6aa599c0d2b844ab82e36f470d94e6af', u'username': u'', u'version': u'5.2'}, 'metadata': {}, 'msg_id': u'ac536491a5c0e203cb886392d4c49f83', 'msg_type': u'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['6aa599c0d2b844ab82e36f470d94e6af']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {u'date': datetime.datetime(2018, 3, 6, 16, 19, 46, 572356, tzinfo=tzlocal()), u'msg_id': u'ac536491a5c0e203cb886392d4c49f83', u'msg_type': u'execute_request', u'session': u'6aa599c0d2b844ab82e36f470d94e6af', u'username': u'', u'version': u'5.2'}, 'metadata': {}, 'msg_id': u'ac536491a5c0e203cb886392d4c49f83', 'msg_type': u'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['6aa599c0d2b844ab82e36f470d94e6af'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {u'date': datetime.datetime(2018, 3, 6, 16, 19, 46, 572356, tzinfo=tzlocal()), u'msg_id': u'ac536491a5c0e203cb886392d4c49f83', u'msg_type': u'execute_request', u'session': u'6aa599c0d2b844ab82e36f470d94e6af', u'username': u'', u'version': u'5.2'}, 'metadata': {}, 'msg_id': u'ac536491a5c0e203cb886392d4c49f83', 'msg_type': u'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u'# select multiple items\\nstart_time = time.tim...batch_lst[:16]))\\n    \\ntime.time() - start_time', store_history=True, silent=False, shell_futures=True)\n   2713                 self.displayhook.exec_result = result\n   2714 \n   2715                 # Execute the user code\n   2716                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2717                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2718                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2719                 \n   2720                 self.last_execution_succeeded = not has_raised\n   2721 \n   2722                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-43-4e73910a79a6>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<ExecutionResult object at 7fb79ce104d0, executi..._before_exec=None error_in_exec=None result=None>)\n   2817 \n   2818         try:\n   2819             for i, node in enumerate(to_run_exec):\n   2820                 mod = ast.Module([node])\n   2821                 code = compiler(mod, cell_name, \"exec\")\n-> 2822                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fb79f7247b0, file \"<ipython-input-43-4e73910a79a6>\", line 4>\n        result = <ExecutionResult object at 7fb79ce104d0, executi..._before_exec=None error_in_exec=None result=None>\n   2823                     return True\n   2824 \n   2825             for i, node in enumerate(to_run_interactive):\n   2826                 mod = ast.Interactive([node])\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fb79f7247b0, file \"<ipython-input-43-4e73910a79a6>\", line 4>, result=<ExecutionResult object at 7fb79ce104d0, executi..._before_exec=None error_in_exec=None result=None>)\n   2877         outflag = 1  # happens in more places, so it's easier as default\n   2878         try:\n   2879             try:\n   2880                 self.hooks.pre_run_code_hook()\n   2881                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2882                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fb79f7247b0, file \"<ipython-input-43-4e73910a79a6>\", line 4>\n        self.user_global_ns = {'Boolean': <class 'sqlalchemy.sql.sqltypes.Boolean'>, 'ClientError': <class 'botocore.exceptions.ClientError'>, 'Column': <class 'sqlalchemy.sql.schema.Column'>, 'DBSCAN': <class 'sklearn.cluster.dbscan_.DBSCAN'>, 'DENOM': 10000, 'Epopcon_db': <class __main__.Epopcon_db>, 'Float': <class 'sqlalchemy.sql.sqltypes.Float'>, 'ForeignKey': <class 'sqlalchemy.sql.schema.ForeignKey'>, 'In': ['', u'from util import *\\nfrom time import gmtime, s...mport connect\\nfrom impala.util import as_pandas', u'class Epopcon_db:\\n    def __init__(self, loca...     else:\\n            return self.wspider_temp', u'epopcon_db = Epopcon_db()\\n\\nwspider_engine = ...engine = epopcon_db.get_engine(production=False)', u'def impute_data(target):\\n    \\n    # cluster ...(query2, [tuple(x) for x in sell_amt_df.values])', u\"conn = connect(host='133.186.168.6', port=21050)\\ncursor = conn.cursor()\", u'\\n\\ndef process_full_batch(batches, save_db=Tr...   logging.warning(\\'done with %s\\' % str(file))', u'ids_df = pd.read_sql_query(\"SELECT ID FROM MWS...RE RELEASE_DT > \\'2018-01-01\\'\", wspider_engine)', u'DENOM = 100\\n# item_ids = ids_df.ID.values\\nit...[(idx, row) for idx, row in enumerate(batch_ls)]', u'len(batch_lst)', u'from joblib import Parallel, delayed', u'# select multiple items\\nidx, query = batch_ls...T\\']] = batch[[\\'STOCK_AMOUNT\\']].astype(np.int)', u\"start_time = time.time()\\n\\n# def process_full...ed_time = time.time() - start_time\\nelapsed_time\", u'4.5 * 16049', u'4.5 * 16049 / 3600', u'(4.5 * 16049 / 3600) / 16', u'DENOM = 100000\\n# item_ids = ids_df.ID.values\\...[(idx, row) for idx, row in enumerate(batch_ls)]', u'len(batch_lst)', u'from joblib import Parallel, delayed', u'start_time = time.time()\\nprocess_full_batch(batch_lst[1])', ...], 'Integer': <class 'sqlalchemy.sql.sqltypes.Integer'>, ...}\n        self.user_ns = {'Boolean': <class 'sqlalchemy.sql.sqltypes.Boolean'>, 'ClientError': <class 'botocore.exceptions.ClientError'>, 'Column': <class 'sqlalchemy.sql.schema.Column'>, 'DBSCAN': <class 'sklearn.cluster.dbscan_.DBSCAN'>, 'DENOM': 10000, 'Epopcon_db': <class __main__.Epopcon_db>, 'Float': <class 'sqlalchemy.sql.sqltypes.Float'>, 'ForeignKey': <class 'sqlalchemy.sql.schema.ForeignKey'>, 'In': ['', u'from util import *\\nfrom time import gmtime, s...mport connect\\nfrom impala.util import as_pandas', u'class Epopcon_db:\\n    def __init__(self, loca...     else:\\n            return self.wspider_temp', u'epopcon_db = Epopcon_db()\\n\\nwspider_engine = ...engine = epopcon_db.get_engine(production=False)', u'def impute_data(target):\\n    \\n    # cluster ...(query2, [tuple(x) for x in sell_amt_df.values])', u\"conn = connect(host='133.186.168.6', port=21050)\\ncursor = conn.cursor()\", u'\\n\\ndef process_full_batch(batches, save_db=Tr...   logging.warning(\\'done with %s\\' % str(file))', u'ids_df = pd.read_sql_query(\"SELECT ID FROM MWS...RE RELEASE_DT > \\'2018-01-01\\'\", wspider_engine)', u'DENOM = 100\\n# item_ids = ids_df.ID.values\\nit...[(idx, row) for idx, row in enumerate(batch_ls)]', u'len(batch_lst)', u'from joblib import Parallel, delayed', u'# select multiple items\\nidx, query = batch_ls...T\\']] = batch[[\\'STOCK_AMOUNT\\']].astype(np.int)', u\"start_time = time.time()\\n\\n# def process_full...ed_time = time.time() - start_time\\nelapsed_time\", u'4.5 * 16049', u'4.5 * 16049 / 3600', u'(4.5 * 16049 / 3600) / 16', u'DENOM = 100000\\n# item_ids = ids_df.ID.values\\...[(idx, row) for idx, row in enumerate(batch_ls)]', u'len(batch_lst)', u'from joblib import Parallel, delayed', u'start_time = time.time()\\nprocess_full_batch(batch_lst[1])', ...], 'Integer': <class 'sqlalchemy.sql.sqltypes.Integer'>, ...}\n   2883             finally:\n   2884                 # Reset our crash handler in place\n   2885                 sys.excepthook = old_excepthook\n   2886         except SystemExit as e:\n\n...........................................................................\n/opt/notebooks/epopcon_lf/<ipython-input-43-4e73910a79a6> in <module>()\n      1 \n      2 \n      3 # select multiple items\n----> 4 start_time = time.time()\n      5 \n      6 Parallel(n_jobs=-1)(map(delayed(wrapper), batch_lst[:16]))\n      7     \n      8 time.time() - start_time\n      9 \n     10 \n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=[(<function wrapper>, ((0, \"('6739668', '6739669', '6739670', '6739671', '67...632', '6749633', '6749634', '6749635', '6749636')\"),), {}), (<function wrapper>, ((1, \"('6749637', '6749638', '6749639', '6749640', '67...601', '6759602', '6759603', '6759604', '6759605')\"),), {}), (<function wrapper>, ((2, \"('6759606', '6759607', '6759608', '6759609', '67...570', '6769571', '6769572', '6769573', '6769574')\"),), {}), (<function wrapper>, ((3, \"('6769575', '6769576', '6769577', '6769578', '67...539', '6779540', '6779541', '6779542', '6779543')\"),), {}), (<function wrapper>, ((4, \"('6779544', '6779545', '6779546', '6779547', '67...508', '6789509', '6789510', '6789511', '6789512')\"),), {}), (<function wrapper>, ((5, \"('6789513', '6789514', '6789515', '6789516', '67...477', '6799478', '6799479', '6799480', '6799481')\"),), {}), (<function wrapper>, ((6, \"('6799482', '6799483', '6799484', '6799485', '67...446', '6809447', '6809448', '6809449', '6809450')\"),), {}), (<function wrapper>, ((7, \"('6809451', '6809452', '6809453', '6809454', '68...415', '6819416', '6819417', '6819418', '6819419')\"),), {}), (<function wrapper>, ((8, \"('6819420', '6819421', '6819422', '6819423', '68...384', '6829385', '6829386', '6829387', '6829388')\"),), {}), (<function wrapper>, ((9, \"('6829389', '6829390', '6829391', '6829392', '68...353', '6839354', '6839355', '6839356', '6839357')\"),), {}), (<function wrapper>, ((10, \"('6839358', '6839359', '6839360', '6839361', '68...322', '6849323', '6849324', '6849325', '6849326')\"),), {}), (<function wrapper>, ((11, \"('6849327', '6849328', '6849329', '6849330', '68...291', '6859292', '6859293', '6859294', '6859295')\"),), {}), (<function wrapper>, ((12, \"('6859296', '6859297', '6859298', '6859299', '68...260', '6869261', '6869262', '6869263', '6869264')\"),), {}), (<function wrapper>, ((13, \"('6869265', '6869266', '6869267', '6869268', '68...229', '6879230', '6879231', '6879232', '6879233')\"),), {}), (<function wrapper>, ((14, \"('6879234', '6879235', '6879236', '6879237', '68...198', '6889199', '6889200', '6889201', '6889202')\"),), {}), (<function wrapper>, ((15, \"('6889203', '6889204', '6889205', '6889206', '68...167', '6899168', '6899169', '6899170', '6899171')\"),), {})])\n    807             if pre_dispatch == \"all\" or n_jobs == 1:\n    808                 # The iterable was consumed all at once by the above for loop.\n    809                 # No need to wait for async callbacks to trigger to\n    810                 # consumption.\n    811                 self._iterating = False\n--> 812             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    813             # Make sure that we get a last message telling us we are done\n    814             elapsed_time = time.time() - self._start_time\n    815             self._print('Done %3i out of %3i | elapsed: %s finished',\n    816                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nHiveServer2Error                                   Tue Mar  6 16:20:33 2018\nPID: 17843                             Python 2.7.14: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/joblib/parallel.pyc in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/opt/notebooks/epopcon_lf/<ipython-input-41-813bb8df29e3> in wrapper(batch=(5, \"('6789513', '6789514', '6789515', '6789516', '67...477', '6799478', '6799479', '6799480', '6799481')\"))\n      4     #     batch = pd.read_sql_query(\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE ITEM_ID in %s\" % query, wspider_engine)\n      5 \n      6 \n      7     stmt = \"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN %s\" % query\n      8 \n----> 9     cursor.execute(stmt)\n     10     batch = as_pandas(cursor)\n     11     batch.columns = [item.upper() for item in batch.columns]\n     12     batch['REG_DT'] = pd.to_datetime(batch['REG_DT'])\n     13     batch[['STOCK_AMOUNT']] = batch[['STOCK_AMOUNT']].astype(np.int)\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in execute(self=<impala.hiveserver2.HiveServer2Cursor object>, operation=\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN...477', '6799478', '6799479', '6799480', '6799481')\", parameters=None, configuration=None)\n    182             self._reset_state()\n    183 \n    184     def execute(self, operation, parameters=None, configuration=None):\n    185         # PEP 249\n    186         self.execute_async(operation, parameters=parameters,\n--> 187                            configuration=configuration)\n    188         log.info('Waiting for query to finish')\n    189         self._wait_to_finish()  # make execute synchronous\n    190         log.info('Query finished')\n    191 \n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in execute_async(self=<impala.hiveserver2.HiveServer2Cursor object>, operation=\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN...477', '6799478', '6799479', '6799480', '6799481')\", parameters=None, configuration=None)\n    200                 self._last_operation_string = operation\n    201             self._last_operation_handle = execute_statement(\n    202                 self.service, self.session_handle, self._last_operation_string,\n    203                 configuration)\n    204 \n--> 205         self._execute_async(op)\n    206 \n    207     def _debug_log_state(self):\n    208         log.debug(\n    209             '_execute_async: self._buffer=%s self._description=%s '\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in _execute_async(self=<impala.hiveserver2.HiveServer2Cursor object>, operation_fn=<function op>)\n    215         # operation_fn should set self._last_operation_string and\n    216         # self._last_operation_handle\n    217         self._debug_log_state()\n    218         self._reset_state()\n    219         self._debug_log_state()\n--> 220         operation_fn()\n    221         self._last_operation_active = True\n    222         self._debug_log_state()\n    223 \n    224     def _reset_state(self):\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in op()\n    198                                                                parameters)\n    199             else:\n    200                 self._last_operation_string = operation\n    201             self._last_operation_handle = execute_statement(\n    202                 self.service, self.session_handle, self._last_operation_string,\n--> 203                 configuration)\n    204 \n    205         self._execute_async(op)\n    206 \n    207     def _debug_log_state(self):\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in wrapper(*args=(<impala._thrift_gen.ImpalaService.ImpalaHiveServer2Service.Client object>, TSessionHandle(sessionId=THandleIdentifier(secre...d='bS3\\xb0\\xc1,@\\x83\\xb8\\xedt\\x80\\x17C\\xa6\\xb1')), \"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN...477', '6799478', '6799479', '6799480', '6799481')\", None), **kwargs={})\n    552                 if six.PY2 and not transport.isOpen():\n    553                     transport.open()\n    554                 elif six.PY3 and not transport.is_open():\n    555                     transport.open()\n    556                 log.debug('Transport opened')\n--> 557                 return func(*args, **kwargs)\n    558             except socket.error:\n    559                 log.exception('Failed to open transport (tries_left=%s)',\n    560                               tries_left)\n    561             except TTransportException:\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in execute_statement(service=<impala._thrift_gen.ImpalaService.ImpalaHiveServer2Service.Client object>, session_handle=TSessionHandle(sessionId=THandleIdentifier(secre...d='bS3\\xb0\\xc1,@\\x83\\xb8\\xedt\\x80\\x17C\\xa6\\xb1')), statement=\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN...477', '6799478', '6799479', '6799480', '6799481')\", configuration=None, async=False)\n    637                                statement=statement, confOverlay=configuration,\n    638                                runAsync=async)\n    639     log.debug('execute_statement: req=%s', req)\n    640     resp = service.ExecuteStatement(req)\n    641     log.debug('execute_statement: resp=%s', resp)\n--> 642     err_if_rpc_not_ok(resp)\n    643     return resp.operationHandle\n    644 \n    645 \n    646 @retry\n\n...........................................................................\n/opt/conda/lib/python2.7/site-packages/impyla-0.11.2-py2.7.egg/impala/hiveserver2.pyc in err_if_rpc_not_ok(resp=TExecuteStatementResp(status=TStatus(errorCode=N...ssages=None, statusCode=3), operationHandle=None))\n    492 \n    493 def err_if_rpc_not_ok(resp):\n    494     if (resp.status.statusCode != TStatusCode.SUCCESS_STATUS and\n    495             resp.status.statusCode != TStatusCode.SUCCESS_WITH_INFO_STATUS and\n    496             resp.status.statusCode != TStatusCode.STILL_EXECUTING_STATUS):\n--> 497         raise HiveServer2Error(resp.status.errorMessage)\n    498 \n    499 \n    500 # datetime only supports 6 digits of microseconds but Impala supports 9.\n    501 # If present, the trailing 3 digits will be ignored without warning.\n\nHiveServer2Error: ArrayIndexOutOfBoundsException: 9\n\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "# select multiple items\n",
    "start_time = time.time()\n",
    "\n",
    "Parallel(n_jobs=-1)(map(delayed(wrapper), batch_lst[:16]))\n",
    "    \n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4871180057525635"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# def process_full_batch(batches, save_db=True, save_img=False, save_fe=True):\n",
    "\n",
    "save_img=False\n",
    "save_db=True\n",
    "save_fe=True\n",
    "# extract features by stock id\n",
    "result_lst = []\n",
    "for idx, group in batch.groupby('ITEM_ID'):\n",
    "#         tmp = list(group_by_item_id.groupby('STOCK_ID'))[0][1]    \n",
    "    result_lst.append(get_feature_engineered_bundle(group))\n",
    "\n",
    "# clean up extracted feature df\n",
    "extracted_feature_df = pd.DataFrame([result for result in result_lst if result != None])\n",
    "\n",
    "\n",
    "try:\n",
    "    # filter dataframe based on extraction criteria\n",
    "    filtered_df, static_item_ids = get_filtered_fg_df(extracted_feature_df)\n",
    "\n",
    "    # filtered df\n",
    "    cleaned_item_ids = filtered_df.item_id.values\n",
    "    cleaned_df = batch[batch['ITEM_ID'].isin(cleaned_item_ids)]\n",
    "\n",
    "    # label extracted feature df\n",
    "    extracted_feature_df['condition_clean'] = 0\n",
    "    extracted_feature_df.loc[extracted_feature_df.item_id.isin(cleaned_item_ids), 'condition_clean'] = 1\n",
    "    extracted_feature_df.loc[extracted_feature_df.item_id.isin(static_item_ids), 'condition_clean'] = 2\n",
    "\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# save images\n",
    "if save_img:\n",
    "    save_img(cleaned_df)\n",
    "\n",
    "# save extracted features to db\n",
    "if save_fe:\n",
    "    pass\n",
    "#         insert_extracted_feature(extracted_feature_df)\n",
    "\n",
    "\n",
    "if save_db:\n",
    "\n",
    "    df_lst =[]\n",
    "    cleaned_df = cleaned_df.sort_values(by=['ITEM_ID', 'STOCK_ID', 'REG_DT'])\n",
    "    for idx, group in list(cleaned_df.groupby('ITEM_ID')):\n",
    "        try:\n",
    "            df_lst.append(get_sell_amount_by_item_id(group))\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "    if len(df_lst) > 0:\n",
    "\n",
    "        result = pd.concat(df_lst)\n",
    "        result[['COLLECT_DAY']] = result.index\n",
    "#             insert_sell_amt(result)\n",
    "#             result.to_sql(con=wspider_temp_engine, name='MWS_COLT_ITEM_SELL_AMT_DEV', if_exists='append')\n",
    "#             logging.warning('done with %s' % str(file))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.253828125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4.5 * 16049 / 3600) / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.59791666666666"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((161 * (127 * 2) / 3600.0) / 16.0) * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "idx, query = batch_lst[0]\n",
    "# batch = pd.read_sql_query(\"SELECT * FROM MWS_COLT_ITEM_IVT WHERE ITEM_ID in %s\" % query, wspider_engine)\n",
    "stmt = \"SELECT * FROM MWS_COLT_ITEM_IVT WHERE item_id IN %s\" % query\n",
    "\n",
    "cursor.execute(stmt)\n",
    "\n",
    "batch = as_pandas(cursor)\n",
    "batch.columns = [item.upper() for item in batch.columns]\n",
    "batch['REG_DT'] = pd.to_datetime(batch['REG_DT'])\n",
    "batch[['STOCK_AMOUNT']] = batch[['STOCK_AMOUNT']].astype(np.int)\n",
    "\n",
    "# extract features by stock id\n",
    "result_lst = []\n",
    "for idx, group in batch.groupby('ITEM_ID'):\n",
    "#     tmp = list(group_by_item_id.groupby('STOCK_ID'))[0][1]    \n",
    "    result_lst.append(get_feature_engineered_bundle(group))\n",
    "\n",
    "# clean up extracted feature df\n",
    "extracted_feature_df = pd.DataFrame([result for result in result_lst if result != None])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # filter dataframe based on extraction criteria\n",
    "filtered_df, static_item_ids = get_filtered_fg_df(extracted_feature_df)\n",
    "\n",
    "# filtered df\n",
    "cleaned_item_ids = filtered_df.item_id.values\n",
    "cleaned_df = batch[batch['ITEM_ID'].isin(cleaned_item_ids)]\n",
    "\n",
    "# label extracted feature df\n",
    "extracted_feature_df['condition_clean'] = 0\n",
    "extracted_feature_df.loc[extracted_feature_df.item_id.isin(cleaned_item_ids), 'condition_clean'] = 1\n",
    "extracted_feature_df.loc[extracted_feature_df.item_id.isin(static_item_ids), 'condition_clean'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert_extracted_feature(extracted_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df_lst = []\n",
    "for stock_id, group_df in list(df.groupby('STOCK_ID')):\n",
    "\n",
    "    imputed_df = impute_data(group_df)[['sell_impute', 'STOCK_AMOUNT', 'STOCK_AMOUNT_imputed_trimed']]\n",
    "    imputed_df['STOCK_ID'] = stock_id        \n",
    "    imputed_df_lst.append(imputed_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df = pd.concat(imputed_df_lst)\n",
    "imputed_df.columns = ['SELL_AMOUNT', 'STOCK_AMOUNT', 'REVISE_STOCK_AMOUNT', 'STOCK_ID']\n",
    "imputed_df['ITEM_ID'] = df.ITEM_ID.values[0]\n",
    "imputed_df['REG_ID'] = reg_id\n",
    "imputed_df['UPT_DT'] = pd.to_datetime(datetime.now(timezone('Asia/Seoul')).strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df['COLLECT_DAY'] = collect_day\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df['UPT_ID'] = 'FILTER ALGO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tttt = list(cleaned_df.groupby('ITEM_ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, group in cleaned_df.groupby('ITEM_ID'):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_lst =[]\n",
    "\n",
    "for idx, group in list(cleaned_df.groupby('ITEM_ID')):\n",
    "#     try:\n",
    "    df_lst.append(get_sell_amount_by_item_id(group))\n",
    "\n",
    "#     except:\n",
    "#         continue\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if len(df_lst) > 0:\n",
    "\n",
    "    result = pd.concat(df_lst)\n",
    "    result[['COLLECT_DAY']] = result.index\n",
    "    insert_sell_amt(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "process_full_batch(batch_lst[0])\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.673159435747398"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((elapsed_time * 15941) / 16) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'HI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel(n_jobs=-1)(map(delayed(process_full_batch), batch_lst))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Parallel(n_jobs=-1)(map(delayed(process_full_batch), batch_lst[1000:2000]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Parallel(n_jobs=-1)(map(delayed(process_full_batch), batch_lst[2000:3000]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Parallel(n_jobs=-1)(map(delayed(process_full_batch), batch_lst[3000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fe5730ab110>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"\"\"REPLACE INTO ADDRESS (USER_ID, EMAIL_ADDRESS) VALUES (3, 'Akaj119@naver.com')\"\"\"\n",
    "# # query = query.replace(\"'\", \"\")\n",
    "\n",
    "# wspider_temp_engine.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"REPLACE INTO MWS_COLT_ITEM_SELL_AMT_DEV %s VALUES %s \"\"\" % (tuple(sell_amt_df.columns), tuple(['%s' for _ in range(len(sell_amt_df.columns))]))\n",
    "#     query = query.replace(\"'\", \"\")\n",
    "#     wspider_temp_engine.execute(query, [tuple(x) for x in sell_amt_df.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENOM = 50\n",
    "# item_ids = ids_df.ID.values[:1000]\n",
    "# n_batches = math.ceil( len(item_ids) / float(DENOM))\n",
    "# batch_ls = [str(tuple(batch)) for batch in np.array_split(item_ids, n_batches)]\n",
    "# batch_lst = [(idx, row) for idx, row in enumerate(batch_ls)]\n",
    "\n",
    "# process_full_batch(batch_lst[5], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[6], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[7], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[8], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[9], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[10], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[11], save_db=True, save_fe=True)\n",
    "# process_full_batch(batch_lst[12], save_db=True, save_fe=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
